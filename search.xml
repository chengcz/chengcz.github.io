<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Whole Genome Amplification Technologies:MALBAC]]></title>
    <url>%2F2018%2F201808-MALBAC-md%2F</url>
    <content type="text"><![CDATA[Technical principles A single cell is picked and lysed. First, genomic DNA of the single cell is melted into single-stranded DNA molecules at 94°C. Picograms of DNA fragments (~10 to 100 kb) MALBAC primers then anneal randomly to single-stranded DNA molecules at 0°C and are extended by a polymerase with displacement activity at elevated temperatures, creating semiamplicons. random primers, each having a common 27-nucleotide sequence and 8 variable nucleotides In the following five temperature cycles, after the step of looping the full amplicons, singlestranded amplicons and the genomic DNA are used as a template to produce full amplicons and additional semiamplicons, respectively. For full amplicons, the 3′ end is complementary to the sequence on the 5′ end. The two ends hybridize to form looped DNA, which can efficiently prevent the full amplicon from being used as a template, therefore warranting a close-to-linear amplification. After the five cycles of linear preamplification, only the full amplicons can be exponentially amplified in the following PCR using the common 27- nucleotide sequence as the primer. PCR reaction will generate microgram level of DNA material for sequencing experiments. Advantages quasi-linear amplification accuracy in CNV detection, especially after signal normalization with a reference cell ###Disadvantages MALBAC is not free from sequence-dependent bias MALBAC is that it has a high false positive rate for SNV detection underamplified regions of the genome are sometimes lost during amplification ref 单细胞全基因组扩增技术: MALBAC文章全文翻译 Zong, Chenghang, et al. “Genome-Wide Detection of Single-Nucleotide and Copy-Number Variations of a Single Human Cell.” Science, vol. 338, no. 6114, 2012, pp. 1622–1626. L, Huang, et al. “Single-Cell Whole-Genome Amplification and Sequencing: Methodology and Applications.” Annual Review of Genomics and Human Genetics, vol. 16, no. 1, 2015, pp. 79–102.]]></content>
      <tags>
        <tag>WGA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Single-cell isoform RNA sequencing (ScISOr-Seq) across thousands of cells reveals isoforms of cerebellar cell types]]></title>
    <url>%2F2018%2F201808_ScISOr-Seq%2F</url>
    <content type="text"><![CDATA[三代平台喜获单细胞测序技能 Methods we employ microfluidics to produce amplified full-length cDNAs barcoded for their cell of origin one pool for 3’ sequencing to measure gene expression(Pacific, Nanopore) another pool for long-read sequencing and isoform expression(10xGenomics) Resultsa.expression We sequence a mean of 17,885 reads per cell After filtering cells and considering only reads confidently mapped to genes, we have 3,875 unique molecular identifiers (UMIs) and 1,448 genes per cell during 3’end sequencing 6,627 cells were clustered into 17 groups To assess stability of clusters, we tripled Illumina sequencing depth for rep2 b.Full-length cDNAs generate ~5.2 million PacBio circular consensus reads cellular barcodes are located close to the polyA-tail, we first searched for polyA-tails chimeras, which were removed from further analysis for 58.0% (compared to 74.0% for 10x-3’seq) of the polyA-tail-containing CCS, we identified a perfect-match 16mer cellular barcode c.ScISOr-Seq using Nanopore sequencing Using 1µg of barcoded cDNA ~31.4% (1D) and ~35.2% (passed 1D2) of Nanopore reads have a 30bp window with &gt;=25 Ts nanopore 需要比 pacbio 更高的样品起始量，nanopore 数据错误率更高 Question 三代数据仍然只是用于转录本组装、注释，需要二代数据定量。分析逻辑约等于三代注释，cell特异转录本，外加二代scRNAseq single cell full-length cDNAs 带 barcode 扩增原理？？？]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>singlecell</tag>
        <tag>pacbio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从 p.HGVS 到基因组变异位置]]></title>
    <url>%2F2018%2F201807_transvar%2F</url>
    <content type="text"><![CDATA[transcvar web 版 transcvar 本地化123456# https://github.com/zwdzwd/transvarpip search transvarpip install transvar --usertransvar config --download_ref --refversion hg19transvar config --download_anno --refversion hg19 12345678910111213head HGVS_cds# MLH1:c.100G&gt;A# MLH1:c.790+1G&gt;A# VHL:c.1_17del17# VHL:c.136G&gt;Ttransvar canno --refversion hg19 --refseq -l HGVS_cds --oneline &gt;HGVS_cds.transvar.txthead HGVS_cds.transvar.txt# input transcript gene strand coordinates(gDNA/cDNA/protein) region info# MLH1:c.100G&gt;A NM_001167618 (protein_coding) MLH1 + chr3:g.37059029G&gt;A/c.100G&gt;A/p.A34T inside_[cds_in_exon_10] CSQN=Missense;reference_codon=GCC;alternative_codon=ACC;dbxref=GeneID:4292,HGNC:7127,HPRD:00390,MIM:120436;aliases=NP_001161090;source=RefSeq# MLH1:c.790+1G&gt;A NM_001258271 (protein_coding) MLH1 + chr3:g.37056036G&gt;A/c.790+1G&gt;A/. inside_[intron_between_exon_9_and_10] CSQN=IntronicSNV;dbxref=GeneID:4292,HGNC:7127,HPRD:00390,MIM:120436;aliases=NP_001245200;source=RefSeq# VHL:c.1_17del17 NM_198156 (protein_coding) VHL + chr3:g.10183532_10183548del17/c.1_17del17/. inside_[cds_in_exon_1] CSQN=CdsStartDeletion;left_align_gDNA=g.10183529_10183545del17;unaligned_gDNA=g.10183532_10183548del17;left_align_cDNA=c.1-3_14del17;unalign_cDNA=c.1_17del17;cds_start_at_chr3:10183532_lost;dbxref=GeneID:7428,HGNC:12687,HPRD:01905,MIM:608537;aliases=NP_937799;source=RefSeq 类似的也可以从 c.HGVS、g.HGVS 开始获得完整注释 本地化数据库配置软件提供数据库路径为：http://transvar.info/transvar_user/annotations/，本地数据库配置详细如下：]]></content>
      <categories>
        <category>bioinfor</category>
      </categories>
      <tags>
        <tag>HGVS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lncRNA 根据染色体位置进行分类]]></title>
    <url>%2F2018%2F201807_lnc_classification%2F</url>
    <content type="text"><![CDATA[1/1/2018 根据与基因的相对位置，lncRNA可以分为Intergenic LncRNAs(lincRNA), Bidirectional LncRNAs, Intronic LncRNAs, Antisense LncRNAs, Sense-overlapping LncRNAs五类，每类详细定义规则如下： 定义ref: https://www.arraystar.com/reviews/v30-lncrna-classification/ Intergenic LncRNAs Intergenic LncRNAs are long non-coding RNAs which locate between annotated protein-coding genes and are at least 1 kb away from the nearest protein-coding genes. They are named according to their 3-protein-coding genes nearby. Gene expression patterns have implicated these LincRNAs in diverse biological processes, including cell-cycle regulation, immune surveillance and embryonic stem cell pluripotency. LincRNAs collaborate with chromatin modifying protein (PRC2, CoREST and SCMX) to regulate gene expression at specific loci. Bidirectional LncRNAs A Bidirectional LncRNA is oriented head to head with a protein-coding gene within 1kb. A Bidirectional LncRNA transcript exhibits a similar expression pattern to its protein-coding counterpart which suggests that they may be subject to share regulatory pressures. However, the discordant expression relationships between bidirectional LncRNAs and protein coding gene pairs have also been found, challenging the assertion that LncRNA transcription occurs solely to “open” chromatin to promote the expression of neighboring coding genes. Intronic LncRNAs Intronic LncRNAs are RNA molecules that overlap with the intron of annotated coding genes in either sense or antisense orientation. Most of the Intronic LncRNAs have the same tissue expression patterns as the corresponding coding genes, and may stabilize protein-coding transcripts or regulate their alternative splicing. Antisense LncRNAs Antisense LncRNAs are RNA molecules that are transcribed from the antisense strand and overlap in part with well-defined spliced sense or intronless sense RNAs. Antisense-overlapping LncRNAs have a tendency to undergo fewer splicing events and typically show lower abundance than sense transcripts. The basal expression levels of antisense-overlapping LncRNAs and sense mRNAs in different tissues and cell lines can be either positively or negatively regulated. Antisense-overlapping LncRNAs are frequently functional and use diverse transcriptional and post-transcriptional gene regulatory mechanisms to carry out a wide variety of biological roles. Sense-overlapping LncRNAs These LncRNAs can be considered transcript variants of protein-coding mRNAs, as they overlap with a known annotated gene on the same genomic strand. The majority of these LncRNAs lack substantial open reading frames (ORFs) for protein translation, while others contain an open reading frame that shares the same start codon as a protein-coding transcript for that gene, but unlikely encode a protein for several reasons, including non-sense mediated decay (NMD) issues that limits the translation of mRNAs with premature termination stop codons and trigger NMD-mediated destruction of the mRNA, or an upstream alternative open reading frame which inhibits the translation of the predicted ORF. 实现12345678910111213141516171819202122232425262728293031323334353637383940414243# step0 prepareawk -F "\t" 'BEGIN&#123;OFS="\t"&#125;&#123;if ($3=="transcript") &#123;print $1,$4,$5,$9,$7&#125;&#125;' lnc.gtf | sed -e 's/gene_id.*transcript_id "//g' -e 's/".*\t/\t.\t/g' &gt;lnc.transcript.bedawk -F "\t" 'BEGIN&#123;OFS="\t"&#125;&#123;if ($3=="transcript") &#123;print $1,$4,$5,$9,$7&#125;&#125;' mRNA.gtf | sed -e 's/gene_id.*transcript_id "//g' -e 's/".*\t/\t.\t/g' &gt;mRNA.transcript.bedawk -F "\t" 'BEGIN&#123;OFS="\t"&#125;&#123;if ($3=="exon") &#123;print $1,$4,$5,$9,$7&#125;&#125;' mRNA.gtf | sed -e 's/gene_id.*transcript_id "//g' -e 's/".*\t/\t.\t/g' &gt;mRNA.exon.bedhead mRNA.transcript.bed# chr1 65419 71585 ENST00000641515 . +# chr1 69055 70108 ENST00000335137 . +# chr1 450703 451697 ENST00000426406 . -# chr1 685679 686673 ENST00000332831 . -# chr1 923928 939291 ENST00000420190 . +# chr1 925150 935793 ENST00000437963 . +# step1 获得 Intergenic lncRNA# -v 取反，获得不和任何 coding-protein 基因（包括基因上下游 1000 bp范围）重合的 lncRNAbedtools window -a lnc.transcript.bed -b mRNA.transcript.bed -v &gt;lnc.Intergenic.bed############### lnc.Intergenic.bed ################ step2 基因 1000 bp 区间内的 lncRNA# 获得和 coding-protein 有重叠的 lncRNA ，lnc_gene.bedbedtools intersect -a lnc.transcript.bed -b mRNA.transcript.bed -wa | sort -u &gt; lnc_gene.bed# 获得 coding-protein 基因上下游 1000 bp 范围内的 lnc ，lnc_gene_1k.bedpython -c 'with open("lnc.transcript.bed") as a, open("lnc_gene.bed") as m, open("lnc.Intergenic.bed") as i, open("lnc_gene_1k.bed", "w") as k1: k1.write("".join(set(a.readlines())-set(m.readlines())-set(i.readlines())))'# step2b 根据转录方向进行分类bedtools window -a lnc_gene_1k.bed -b mRNA.transcript.bed -u -Sm | sort -u &gt;lnc.Bidirectional.bed############### lnc.Bidirectional.bed ###############bedtools window -a lnc_gene_1k.bed -b mRNA.transcript.bed -u -sm | sort -u &gt;lnc.Enhancer.bed############### lnc.Enhancer.bed ################ step3 # 和 coding-protein 重叠，不和外显子重叠，得到内含子区域的lncbedtools intersect -a lnc_gene.bed -b mRNA.exon.bed -v &gt; lnc.Intronic.bed############### lnc.Intronic.bed ################ 取相同链的 lncbedtools intersect -a lnc_gene.bed -b mRNA.exon.bed -wa -s | sort -u &gt; lnc.Sense.bed############### lnc.Sense.bed ################ 取反向链的 lncbedtools intersect -a lnc_gene.bed -b mRNA.exon.bed -wa -S | sort -u &gt; lnc.Antisense.bed############### lnc.Antisense.bed ###############]]></content>
      <categories>
        <category>RNAseq</category>
      </categories>
      <tags>
        <tag>lncRNA</tag>
        <tag>classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[家族基因分析的一般套路]]></title>
    <url>%2F2018%2F201807_FamilyGene%2F</url>
    <content type="text"><![CDATA[7/2016 分析前准备 分析前准备主要是两点，找文章，找序列。 找文章，该家族基因是否在研究物种或其他物种中报告，都进行了什么分析和试验； 找序列，获取该家族代表性序列进行后续分析 家族基因成员鉴定 主要 blast 和 hmmer 两种方式，一般讲来使用两个方式的并集，然后根据基因特征再一步确认过滤 blast hmmer 家族基因特征描述 染色体分布 基因结构 motif分析 启动子序列分析 多物种进化关系分析 构建分子进化树 Ka/Ks计算等等 基因分类命名 一般根据进化关系、或者分析特征进行分类，然后染色体顺序命名 基因表达分析 多组织材料表达分析 不同处理样品表达分析 共表达分析 候选基因表达、功能验证 123456789101112graph LRA[FamilyGene.seq] -- blast --&gt; B[FamilyGene.list]A[FamilyGene.hmm] -- hmmer --&gt; B[FamilyGene.list]B[FamilyGene.list] --&gt; C[FeatureAnalysis]C[FeatureAnalysis] --&gt; Ca[染色体分布]C[FeatureAnalysis] --&gt; Cb[基因结构]C[FeatureAnalysis] --&gt; Cc[motif分析]C[FeatureAnalysis] --&gt; Cd[promotor seq analysis]B[FamilyGene.list] --&gt; D[phylogenetics]B[FamilyGene.list] --&gt; E[Group,Name]B[FamilyGene.list] --&gt; F[Expression]F[Expression] --&gt; G[表达, 功能验证]]]></content>
      <categories>
        <category>bioinfor</category>
      </categories>
      <tags>
        <tag>familygene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Global characterization of T cells in non-small-cell lung cancer by single-cell sequencing]]></title>
    <url>%2F2018%2F201807_singleTcell_NSCLC%2F</url>
    <content type="text"><![CDATA[肺癌T细胞 scRNAseq 分析 Methods we performed deep single-cell RNA sequencing on T cells isolated from tumor, adjacent normal tissues and peripheral blood for 14 treatment-naïve patients, including 11 adenocarcinomas and three squamous cell carcinomas After confirming the existence of T cell infiltration in NSCLC tumors (Supplementary Fig. 1a), we sorted various T cell subtypes based on cell surface markers CD3/ CD4/CD8/CD25 by fluorescence-activated cell sorting A total of 12,346 cells were sequenced at an average depth of 1.04 million uniquely-mapped read pairs per cell The final expression table contained 12,415 genes and 11,769 cells Dimensional reduction analysis (t-SNE) applied to the expression data revealed that T cells clustered primarily based on their tissue origins and subtypes To further address the intrinsic T cell heterogeneity, we applied unsupervised clustering based on t-SNE+ densityClust and identified seven CD8 and nine CD4 clusters To gain insight into the clonal relationship among individual T cells, we reconstructed their T cell receptor (TCR) sequences We obtained full-length TCRs with both alpha and beta chains for 8,038 cells of the 16 clusters, including 5,015 harboring unique TCRs and 3,023 harboring repeated use of TCRs, indicative of clonal expansion Detection of Treg marker genes Survival analysis. Results As well as tumor-infiltrating CD8+ T cells undergoing exhaustion, we observed two clusters of cells exhibiting states preceding exhaustion, and a high ratio of “pre-exhausted” to exhausted T cells was associated with better prognosis of lung adenocarcinoma. we observed further heterogeneity within the tumor regulatory T cells (Tregs), characterized by the bimodal distribution of TNFRSF9, an activation marker for antigenspecific Tregs. The gene signature of those activated tumor Tregs, which included IL1R2, correlated with poor prognosis in lung adenocarcinoma. … points 通常，CX3CR1在正常组织中表达高于肿瘤组织]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>singlecell</tag>
        <tag>cancer</tag>
        <tag>scRNAseq</tag>
        <tag>TCR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[identification of A-to-I RNA Editing]]></title>
    <url>%2F2018%2F201807_RNAedit%2F</url>
    <content type="text"><![CDATA[Quality Contorl of Fastq Alignment to Reference Genome Recommended use STAR-2pass to mapping reads to genome Pre-process of Bam File remove multi aglin, duplicate reads, splitN, and so on Detailed operation can refer: https://software.broadinstitute.org/gatk/documentation/article.php?id=3891 use GATK to call candidate site Basic Variant filtering(optional) Identification of editing sites Select single nucleotide variants(SNV) from all variant results , use snpEff to annot SNV Select A&gt;T changes in the gene interval, depending on the direction of gene transcription Remove sites that appear in Cosmic, dbSNP, TCGA somatic mutations database Remove sites which the editing level is 1.0 in all sample At least 10 reads in at least 1 sample RNA editing sites is exists at least 2 sample ref： The Genomic Landscape and Clinical Relevance of A-to-I RNA Editing in Human Cancers Dynamic regulation of RNA editing in human brain development and disease A disrupted RNA editing balance mediated by ADARs (Adenosine DeAminases that act on RNA) in human hepatocellular carcinoma ADAR1 promotes malignant progenitor reprogramming in chronic myeloid leukemia Genome-wide analysis of A-to-I RNA editing by single-molecule sequencing in Drosophila]]></content>
      <categories>
        <category>RNAseq</category>
      </categories>
      <tags>
        <tag>RNAediting</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[转:16S基本分析点]]></title>
    <url>%2F2018%2F201807_16S%2F</url>
    <content type="text"><![CDATA[8/23/2016 8:35 PM 基本介绍为什么选择16S测序？细菌中唯一的细胞器是核糖体，它的沉降系数是70S，由50S的大亚基和30S的小亚基组成，其中rRNA按沉降系数可分为5S、16S和23S三种，5S和23S rRNA在核糖体大亚基中，16S rRNA在核糖体小亚基中。5S rRNA的序列长度最长达400nt，信息量少；23S rRNA的序列长度长达2900nt，序列太长，测序通量、深度等要求高；而16S rRNA序列长度适中，约为1542nt。沉降系数：离心法时，大分子沉降速度的量度，等于每单位离心场的速度。沉降系数10-13秒称为一个Svedberg单位，简写S，量纲为秒。 16S测序区域如何选择？16S rDNA是编码16S rRNA的DNA序列，存在于所有的细菌和古菌的基因组中，一般由9个保守区和9个可变区组成，保守区在细菌间无显著差异，可用于构建所有生命的统一进化树，而可变区在不同细菌中存在一定的差异，可将菌群鉴定精细到分类学上属，甚至种的级别。 16S rDNA测序区域与哪些因素有关？ 引物序列， 16S目的片段的引物是基于保守区的序列设计的，但是由于保守序列的碱基存在多态性，可能会忽略一部分微生物，因此要选用覆盖率高的引物进行目的片段的扩增； 测序平台， Illumina MiSeq/HiSeq测序平台的架构决定其短读长，它的读长最长为2*300pb，限制16S只能进行单V区、双V区或者三V区的测定； 目的区域中已知序列的多少， 扩增的目的片段与数据库中已知的片段进行比对，如果数据库中该区域已知片段很少，很不全面，导致测得的序列比对不上，微生物的多样性随之降低； 不同区域中鉴定物种的准确性， 不同区域的物种比对到RDP数据库时，鉴定物种的准确性也是测序区域选择的约束条件。单个区域的检测发现，V2区和V4区在各个水平上的精确度最高，V5、V6、V7、V8区在门、纲水平上的精确度也较高，与V2区和V4区相似。 OTU是什么OTU（operational taxonomic units），即操作分类单元。通过一定的距离度量方法计算两两不同序列之间的距离度量或相似性，继而设置特定的分类阈值，获得同一阈值下的距离矩阵，进行聚类操作，形成不同的分类单元。 OTU在16S测序中有何用高通量测序得到的16S序列有成千上万条，如果对每条序列都进行物种注释的话，工作量大、耗时长，而且16S扩增、测序等过程中出现的错误会降低结果的准确性。在16S分析中引入OTU，首先对相似性序列进行聚类，分成数量较少的分类单元，基于分类单元进行物种注释。这不仅简化工作量，提高分析效率，而且OTU在聚类过程中会去除一些测序错误的序列，提高分析的准确性。 OTU如何聚类OTU聚类的方法多种多样，如Uclust、cd-hit、BLAST、mothur、usearch和 prefix/suffix，这些聚类方法均可以在QIIME软件中实施。不同聚类方法基于不同的算法，得到的聚类结果虽然不同，但是大体的聚类流程都是一致的，挑选非重复序列，与16S数据库比对，去除嵌合体序列，序列之间距离计算，97%相似OTU聚类。 嵌合体序列是RCR扩增时，两条不同的序列产生杂交、扩增的序列 OTU跟物种的关系OTU聚类后，挑选出每个OTU中的代表序列，与RDP、Sliva或GreenGene等数据库进行比对，进行物种注释。OTU和物种是映射关系，它们一一对应或多对一关系 Alpha多样性Alpha多样性：指一个区域或生态系统内的多样性，用来描述单个样品的物种多样性；Alpha多样性指数主要用来表征三方面信息：物种丰度、物种多样性和测序深度。 物种丰度 observed_species指数：表示实际观测到的OTU数量； chao1指数：评估样品中所含OTU的总数。其公式为： Schao1：估计的OTU数量；Sobs：实际观察到的OTU数量；n1：只含一条序列的OTU的数量；n2：含两条序列的OTU的数量。 物种多样性 Shannon指数：包含着种数和各种间个体分配的均匀性两个部分。如果每一个体都属于不同的种，多样性指数就最大；如果每一个体都属于同一种，则其多样性指数就最小。用来估算微生物群落的多样性，shannon值越大，物种多样性越高。其计算公式： Sobs：实际观察到的OTU数量；ni：第i条OTU的序列数量；N：所有的序列数。 Simpson指数：随机抽取的两个个体属于不同种的概率，Simpson指数越大，物种多样性越高。其计算公式： Sobs：实际观察到的OTU数量；ni：第i个OTU的序列数量；N：所有的序列数。 3 . 测序深度 Coverage：反应测序深度，goods_coverage 指数越接近于1，说明测序深度已经基本覆盖到样品中所有的物种。其计算公式： Cdepth：goods_coverage指数表示测序深度；n1：只有含一条序列的OTU数目；N：为抽样中出现的总的序列数。 如何展现Alpha多样性指数Alpha多样性指数数值可以以表格的形式展现，还可以以图表的形式展现。随着抽取的reads条数的增加，曲线逐渐趋平，表明测序量是足够的，可以覆盖样品中的大部分微生物；如果曲线呈现上升趋势，则需要增加测序量，保障测序结果的代表性。简单来说，其实Alpha多样性指数就是衡量测序量是否足够的一个标准。它们大都长成这个样子。 Beta多样性Beta多样性：指不同生态系统之间的多样性比较，用来比较组间样品在物种多样性上存在的差异。Beta多样性是基于不同样品序列间的进化关系及丰度信息来计算样品间距离，用来描述不同样品间的相似性和差异性。样本间距离是指样本之间的相似程度，可以通过数学方法估算。如前所述，样本间越相似，距离数值越小。计算微生物群体样本间距离的方法有多种，例如，Jaccard、Bray-Curtis、Unifrac等。这些距离算法主要分为两大类别：OTU间是否关联；OTU是否加权(表) \ 基于独立OUT 基于系统发生数 加权 Bray-Curtis Weighted Unifrac 非加权 Jaccard Unweighted Unifrac 基于独立OTU vs 基于系统发生树二代测序当中，我们对16s rDNA某个区域进行测序后，会根据序列的相似度定义OTU。这个时候，基于独立OTU的计算方式认为OTU之间不存在进化上的联系，每个OTU间的关系平等。而基于系统发生树计算的方法，会根据16s的序列信息对OTU进行进化树分类，因此不同OTU之间的距离实际上有“远近”之分。 加权 vs 非加权利用非加权的计算方法，主要考虑的是物种的有无，即如果两个群体的物种类型都一致，表示两个群体的β多样性最小。而加权方法，则同时考虑物种有无和物种丰度两个问题。如果A群体由3个物种a和2个物种b组成，B群体由2个物种a和3个物种b组成，则通过非加权方法计算，因为A群体与B群体的物种组成完全一致，都只由物种a和b组成，因此它们之间的β多样性为0。但通过加权方法计算，虽然A与B群体的组成一致，但物种a和b的数目却不同，因此两个群体的β多样性则并非一致。 在宏基因组和16s测序的分析中，使用最多的距离算法主要有Bray-Curtis和Weighted 及Unweighted Unifrac。因此，下面我们就这几种常用的微生物多样性算法的特点和应用范围进行简单比较。 Bray-Curtis距离 vs Unifrac距离Bray-Curtis距离和Unifrac距离的主要区别在于计算β值的时候是否考虑OTU的进化关系。根据表2，显然，只有后者是有考虑。这会影响到它们的：数值表述意义不同：虽然两种方法的数值都是在0-1之间，但具体所表示的生物学意义却不一样。在Bray-Curtis算法中，0表示两个微生物群落的OTU结构（包括组成和丰度）完全一致；而在Unifrac中，0更侧重于表示两个群落的进化分类完全一致；实际应用的合理性：在实际微生物研究中，如果样本间物种的近源程度较高（温和处理样本与对照样本，生境相似的不同样本等），利用Bray-Curtis这种把OTU都同等对待的方法，更有利于发现样本间的差异。而Unifrac则更适合用于展示此类样本的重复性。 Weighted Unifrac距离 vs Unweighted Unifrac距离Unifrac除了具有考虑OTU之间的进化关系的特点之外，根据有没有考虑OTU丰度的区别，Unifrac分析可以分为加权（WeightedUunifrac）和非加权（Unweighted Unifrac）两种方法。它们的不同在于：数值表述意义：Unweighted UniFrac只考虑了物种有无的变化，因此结果中，0表示两个微生物群落间OTU的种类一致。而Weighted UniFrac则同时考虑物种有无和物种丰度的变化，结果中的0则表示群落间OTU的种类和数量都一致。实际应用的合理性：在环境样本的检测中，由于影响因素复杂，群落间物种的组成差异更为剧烈，因此往往采用非加权方法进行分析。但如果要研究对照与实验处理组之间的关系，例如研究短期青霉素处理后，人肠道的菌落变化情况，由于处理后群落的组成一般不会发生大改变，但群落的丰度可能会发生大变化，因此更适合用加权方法去计算。 多样性分析方法 Unifrac分析基于系统进化，在 OTU水平上反应不同组间微生物群落结构的差异。若两个微生物群落完全相同，它们是没有各自独立的进化过程，UniFrac的值为0；UniFrac值越大，说明不同微生物群落在进化过程中变异越大，两个微生物群落在进化树中完全分开，则它们是两个完全独立的进化过程，UniFrac值最大，为1。 图A和图B是Unifrac的两种展现形式，样品越靠近说明两个样品的组成越相似。 PCoA分析PCoA（principal coordinate analysis）主坐标分析，基于降维的方法，在尽可能保留原始信息的情况下，找出前两种影响分组的信息，研究其对样品分组的贡献率，将样品的相似性或差异性可视化，这两个坐标轴是没有任何实际意义的。分析结果展示如下，如果两个样品间距离较近，则表示这两个样品的物种组成比较相近。 横坐标即第一主坐标，表示对样品分开的贡献率是60.91%；纵坐标即第二主坐标，表示对样品分开的贡献率是14.06%。箱线图的添加是本图的一个特色，直观地展现样品在第一主坐标和第二主坐标的分布情况。 NMDS分析NMDS与PCoA分析的意义一样，都 是用来展示样品间差异和相似的方法，如果两个样品距离较近，则表示这两个样品的物种组成比较相近。不同之处是两种展现形式是基于不同的算法。 横纵坐标是基于进化或者数量距离矩阵的数值在二维表中成图，不同颜色代表不同分组，点代表样品，点与点之间的距离表示差异程度。 Anosim分析相似性分析Anosim分析是一种非参数检验，用来检验组间(两组或多组)的差异是否显著大于组内差异，从而判断分组是否有意义。首先利用Bray-Curtis算法计算两两样品间的距离，然后将所有距离从小到大进行排序。 横坐标表示所有样品(Between)以及每个分组(A、B)，纵坐标表示unifrac距离的秩。Between组比其它每个分组的秩较高时，则表明组间差异大于组内差异。R介于(-1，1)之间，R大于0，说明组间差异显著；R小于0，说明组内差异大于组间差异，统计分析的可信度用P表示，P&lt; 0.05表示统计具有显著性。 UPGMA层次聚类假设在进化过程中所有核苷酸(氨基酸)的变异率相同，基于群落组成和结构的算法计算样本间的距离，根据β多样性距离矩阵进行层次聚类分析，最后通过UPGMA构建系统进化树。此树可以直观的反应样本间进化的差异。 UPGMA层次聚类分析。树枝不同颜色代表不同的分组。 Beta多样性分析还有什么要点？上面的Unifra、PCoA、NMDS、Anosim和UPGMA分析均可做考虑物种丰度（加权，Weight）和不考虑物种丰度（非加权，Unweight）的两种展现形式，可以根据你的研究目的选择合适的计算方法。]]></content>
      <categories>
        <category>microbe</category>
      </categories>
      <tags>
        <tag>amplicon</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib 画图的 Qt 问题]]></title>
    <url>%2F2018%2F201807_matplotlib%2F</url>
    <content type="text"><![CDATA[This application failed to start because it could not find or load the Qt platform plugin “xcb” in “”. 1export QT_PLUGIN_PATH="/data/software/anaconda2/plugins/" QXcbConnection: Could not connect to display 方法一： 设置环境变量 1export QT_QPA_PLATFORM='offscreen' 方法二： python 画图脚本添加下面两行 12import matplotlibmatplotlib.use('Agg')]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[canonical transcripts of Human protein coding gene]]></title>
    <url>%2F2018%2F201806_HumanGene%2F</url>
    <content type="text"><![CDATA[我们使用 snpEff, annovar 对得到的变异集注释，总会得到多个转录本注释结果，而只需要按照 HGVS 规范报告一个经典转录本(canonical transcripts) 对应注释结果，这时候就需要知道每个基因对应的经典转录本。 一般情况下，我们使用基因对应的最长转录本代表该基因，具体操作上，首先从 HGNC 下载经过确认的基因对应经典转录本信息，其中不确定基因再使用最长转录本代表。 HGNC 下载编码蛋白基因对应转录本 1# wget -c ftp://ftp.ebi.ac.uk/pub/databases/genenames/new/tsv/locus_types/gene_with_protein_product.txt 根据表格，很方便得到基因名字对应 hgnc_id, entrez_id, ensembl_gene_id, refseq_accession 等多数据库基因ID，使用 refseq_accession 进行 HGVS 表示，这里就只关注 refseq_accession ， 还是可以发现其中一些基因名并不能对应到唯一 refseq_accession，下面就需要我们进行第二步，结合 GFF 文件挑出经典转录本。 NCBI下载人类基因注释GFF文件 1# wget -c ftp://ftp.ncbi.nlm.nih.gov/refseq/H_sapiens/annotation/GRCh37_latest/refseq_identifiers/GRCh37_latest_genomic.gff.gz 使用脚本将 gff 转换为表格样式，方便筛选每个基因对应最长转录本，具体操作参考如下，其中 pyGTF 为用来解析 gff 文件。 输出文件可以下载 , 其具体格式为： 表头分别是：gene_id, gene_name, gene_type, transcript_id, transcript_name, transcript_type, transcript_length, chromosome, start, end, strand，根据 gene_name, transcript_name, transcript_length 三列排序，就可以很方便筛选出基因对应最长转录本。 脚本整合 HGNC 和 GFF 两部分结果 123456789101112131415161718192021222324252627282930import pandas as pdgene, length = &#123;&#125;, &#123;&#125;gff = pd.read_table('hg37.geneinfo.txt', header=None)gff = gff[(gff[2]=='protein_coding') | (gff[5]=='protein_coding')]nrow, ncol = gff.shapefor index in range(nrow): transcriptid = gff.iloc[index,3] try: transcriptid = transcriptid[:transcriptid.rindex('.')] except: pass gene.setdefault(gff.iloc[index,1], []).append(transcriptid) length[transcriptid] = gff.iloc[index,6]hgnc = pd.read_table('gene_with_protein_product.txt')hgnc['symbol'] = hgnc['symbol'].astype(str)hgnc['refseq_accession'] = hgnc['refseq_accession'].astype(str)nrow, ncol = hgnc.shapefor index in range(nrow): genename = hgnc.loc[index, 'symbol'] refseq = hgnc.loc[index, 'refseq_accession'] tmp = refseq.split('|') if len(tmp) == 1: print('&#123;&#125;\t&#123;&#125;\thgnc'.format(genename, refseq)) else: if len(tmp) == 0: tmp = gene[genename] tmp = sorted(tmp, key=lambda x: length.get(x, 0), reverse=True) print('&#123;&#125;\t&#123;&#125;\tgff'.format(genename, tmp[0])) 结果整理文件可下载：https://github.com/chengcz/BioResource/tree/master/HumanGene/protein_coding_gene_canonical_transcripts.txt]]></content>
      <categories>
        <category>bioinfor</category>
      </categories>
      <tags>
        <tag>gtf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 python 正确读取不同编码方式文本]]></title>
    <url>%2F2018%2F201806_python_decode%2F</url>
    <content type="text"><![CDATA[通常情况下，都会默认输入文件以 utf-8 的方式进行编码，总会有几个奇葩输入文件存在，导入分析流程，啊额，编码问题运行终止，再来一遍呢还是再来一遍。 技术问题还是选择用技术手段解决好些，是吧，一个简单的例子如下： 123456789101112131415161718192021from io import opendef Read_File_Right_Encode(fp, encodelst=None): ''' ''' # content = subprocess.check_output('iconv -f GBK -t utf-8 &#123;file&#125;') with open(fp, 'rb') as f: tmp = f.read() if encodelst is None: encodelst = ['gbk', 'ascii', 'utf-8'] x = 0 while True: if x == len(encodelst): raise Exception('Can\'t find right coding format of &#123;&#125; in decode list: &#123;&#125;'.format(fp, ', '.join(encodelst))) try: content = tmp.decode(encodelst[x]) break except: pass x += 1 return content]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>decode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NA12878 全外显子数据分析结果比较]]></title>
    <url>%2F2018%2F201806_compare_vcf%2F</url>
    <content type="text"><![CDATA[一直都比较有兴趣分析 human DNA 变异，参照 gatk 流程分析了 NA12878 的全外数据，正好有参考变异集，比较下分析准确性，也好参数调整优化之类，吧吧吧，下面就是一些详细步骤粘贴整理 NA12878.vcf 处理123456789101112131415# wget -c ftp://ftp-trace.ncbi.nih.gov/giab/ftp/data/NA12878/Garvan_NA12878_HG001_HiSeq_Exome/project.NIST.hc.snps.indels.vcfhc_snp_indel=project.NIST.hc.snps.indels.vcf# wget -c ftp://ftp-trace.ncbi.nih.gov/giab/ftp/data/NA12878/Garvan_NA12878_HG001_HiSeq_Exome/nexterarapidcapture_expandedexome_targetedregions.bedintervals=nexterarapidcapture_expandedexome_targetedregions.bedjava -jar GenomeAnalysisTK.jar \ -T SelectVariants \ -R $genome \ -V $hc_snp_indel \ -L $intervals \ -sn NIST7035 \ -select 'DP &gt; 5' \ --excludeNonVariants \ -o ref.NIST7035.hc.snps.indels.vcf.gz# 选择特定区间内 DP 大于5，样品 NIST7035 的变异位点 vcf 处理ref: https://software.broadinstitute.org/gatk/documentation/article.php?id=2806 12345678910111213141516171819202122232425262728293031323334353637383940414243# SNPjava -jar GenomeAnalysisTK.jar \ -T SelectVariants \ -R $genome \ -L $intervals \ -selectType SNP \ -V NIST7035.vcf.gz \ -o NIST7035.snp.vcf.gzjava -jar GenomeAnalysisTK.jar \ -T VariantFiltration \ -R $genome \ --filterExpression "DP &lt; 5" --filterName "LowDepth" \ --filterExpression "FS &gt; 60.0 || SOR &gt; 3.0" --filterName "StrandBias" \ --filterExpression "QD &lt; 2.0 || MQ &lt; 40.0 || MQRankSum &lt; -12.5" \ --filterName "snp_Filter" \ --filterExpression "ReadPosRankSum &lt; -8.0" --filterName "endOfRead" \ -V NIST7035.snp.vcf.gz \ -o NIST7035.snp.filter.vcf.gz# Indeljava -jar GenomeAnalysisTK.jar \ -T SelectVariants \ -R $genome \ -L $intervals \ -selectType INDEL \ -V NIST7035.vcf.gz \ -o NIST7035.indel.vcf.gzjava -jar GenomeAnalysisTK.jar \ -T VariantFiltration \ -R $genome \ --filterExpression "DP &lt; 5" --filterName "LowDepth" \ --filterExpression "QD &lt; 2.0 || FS &gt; 200.0" \ --filterName "InDels_Filter" \ --filterExpression "ReadPosRankSum &lt; -20.0" --filterName "endOfRead" \ -V NIST7035.indel.vcf.gz \ -o NIST7035.indel.filter.vcf.gz# MergeVcfjava -jar picard.jar MergeVcfs \ I=NIST7035.indel.filter.vcf.gz \ I=NIST7035.snp.filter.vcf.gz \ O=NIST7035.filter.vcf.gz\ D=$genome.dict compare use bcftools12345678910111213141516171819202122bcftools isec NIST7035.filter.vcf.gz ref.NIST7035.hc.snps.indels.vcf.gz -p bcftools# ref.vcf.gz, alt.vcf.gz 使用 bgzip 压缩，同时 tabix 进行 index# output: README.txt# This file was produced by vcfisec.# The command line was: bcftools isec -p bcftools NIST7035.filter.vcf.gz ref.NIST7035.hc.snps.indels.vcf.gz## Using the following file names:# bcftools/0000.vcf for records private to NIST7035.filter.vcf.gz# bcftools/0001.vcf for records private to ref.NIST7035.hc.snps.indels.vcf.gz# bcftools/0002.vcf for records from NIST7035.filter.vcf.gz shared by both NIST7035.filter.vcf.gz ref.NIST7035.hc.snps.indels.vcf.gz# bcftools/0003.vcf for records from ref.NIST7035.hc.snps.indels.vcf.gz shared by both NIST7035.filter.vcf.gz ref.NIST7035.hc.snps.indels.vcf.gzcat ref.NIST7035.hc.snps.indels.vcf.gz | grep -v '^#' | wc -l # 54077cat 0000.vcf | grep -cv '^#' # 3863, 假阳性cat 0001.vcf | grep -cv '^#' # 2131, 假阴性cat 0002.vcf | grep -cv '^#' # 51946cat 0003.vcf | grep -cv '^#' # 51946cat 0000.vcf| grep -v '^#' | grep PASS | wc -l # 1334cat 0002.vcf| grep -v '^#' | grep PASS | wc -l # 47143 不考虑snp、indels过滤情况下，计算假阳性、假阴性率。 假阳性: 3863/54077 = 7.14 % 假阴性: 2131/54077 = 3.94 % 哈哈哈哈，瞎了，没法看的样子。还是考虑下变异过滤结果吧 假阳性: 1334/54077 = 2.46 % 假阴性: (2131+51946-47143)/54077 = 12.82 % 更加让人没话说，这个假阴性太感人，过滤标准不太合适啊 123456789# 先看被过滤掉的这部分阴性cat 0002.vcf | grep -v '^#' | grep -v PASS | awk '&#123;n[$7]+=1&#125;END&#123;for (i in n)&#123;print i,n[i]&#125;&#125;'# LowDepth 11# StrandBias;snp_Filter 277# LowDepth;snp_Filter 2# InDels_Filter 265# StrandBias 3098# LowDepth;StrandBias 2# snp_Filter 1148 过滤掉位点最严重的两项分别是StrandBias, snp_Filter，对应过滤指标分别为FS &gt; 60.0 || SOR &gt; 3.0, QD &lt; 2.0 || MQ &lt; 40.0 || MQRankSum &lt; -12.5，转换vcf文件到tab格式（脚本：github），对其中一些指标进行分布检查。 先看 StrandBias，对 FS, SOR 两个参数分析，分布结果如下，可看出被过滤的位点基本上都是由于 SOR 参数（作图时FS&gt;60无显示），与 NA12878 变异集相比，需调整 SOR 临界值到更大水平。 再看 snp_Filter 对应的QD &lt; 2.0 || MQ &lt; 40.0 || MQRankSum &lt; -12.5，其中 MQRankSum 对应67个位点，QD 对应557个位点，MQ 对应 1168 个位点，看出 MQ, QD 标准需要进行调整。 以现在质控指标对文件0001.vcf 2131 变异质控，QD 过滤掉949，MQRankSum 过滤掉14，MQ 过滤掉195，FS 过滤掉135，根据这部分参考变异集质量指标，同样 QD, MQ, FS 参数需要进行调整。 另，vcftools 也可以很方便进行两个 vcf 文件的比较，还没 bcftools 对 vcf 文件必须排序、index的要求，输出结果也相应不同，因为我需要看阳性、阴性位点是否被过滤以及过滤情况，就不再用 vcftools 整理了，具体命令可参考如下： 1234# http://vcftools.github.io/vcftools --vcf ref.vcf --diff alt.vcf --out prefix# ref.vcf.gz # 则需要使用--gzvcf# alt.vcf.gz # 则需要使用--gzdiff 质控参数调整思路根据质控结果比较来看，参控参数都还需要进行再调整，一个初步的参数调整思路是，对高质量变异集各项质控指标分布进行统计，然后进行阈值选择；或者对 NA12878 全基因组水平的变异进行 VQSR 分析，统计高质量变异位点质控指标分布]]></content>
      <categories>
        <category>WES</category>
      </categories>
      <tags>
        <tag>NA12878</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vcf 注释 —— ANNOVAR]]></title>
    <url>%2F2018%2F201806_ANNOVAR%2F</url>
    <content type="text"><![CDATA[最先知道的 VCF 注释软件，但是呢没有搞定构建研究物种的注释数据库，于是转向snpEff，工作转向临床分析时候，发现 ANNOVAR 在人类数据注释多种第三方数据库支持，变异频率、HGVS、ACMG致病性、dbSNP、Cosmic支持等等 软件文档镇楼: http://annovar.openbioinformatics.org/en/latest/ 软件下载ANNOVAR 由 perl 实现，下载即用，软件下载地址为：http://www.openbioinformatics.org/annovar/annovar_download_form.php ，软件包包括下面几个功能脚本： annotate_variation.pl, 主程序，数据库下载，变异注释等等 coding_change.pl, 推断蛋白序列的变化 convert2annovar.pl, retrieve_seq_from_fasta.pl, table_annovar.pl, 注释程序，根据数据库选择完成不同类型变异注释 variants_reduction.pl, 数据库下载、整理ANNOVAR 注释变异可以分成有基于基因、基于染色体区间和变异数据等三种类型 基于gene的注释 注释结果为突变位点位于基因的相对位置，是否改变氨基酸编码，获得变异位点的HGVS命名方式 基于染色体区间的注释 获取变异位点是否存在于某些特定的区间内，Identify cytogenetic band, 转录因子结合区等等 变异数据库的注释 包括Clinvar, dbSNP, Cosmic, ExAC, gnomAD等等，突变数据库整理可参考从 vcf 文件准备 ANNOVAR 数据库 ANNOVAR 数据库文件实际上为特定格式的文本文件，其数据库文件命名规则为: \${path_database}/\${buildver}_\${database_name}.txt 软件运行1234# input file format: vcftable_annovar.pl example/ex2.vcf humandb/ -buildver hg19 -out myanno -remove -protocol refGene,cytoBand,esp6500siv2_all,1000g2015aug_all,1000g2015aug_afr,1000g2015aug_eas,1000g2015aug_eur,snp138,dbnsfp30a -operation g,r,f,f,f,f,f,f,f -nastring . -vcfinput# 不同于snpEff，ANNOVAR 所有注释结果都在 vcf 文件 INFO 列添加key-value 非人物种数据库整理12345678910111213# 下载物种基因序列、注释文件wget -c ftp://ftp.ensemblgenomes.org/pub/release-27/plants/fasta/arabidopsis_thaliana/dna/Arabidopsis_thaliana.TAIR10.27.dna.genome.fa.gzwget -c ftp://ftp.ensemblgenomes.org/pub/release-27/plants/gtf/arabidopsis_thaliana/Arabidopsis_thaliana.TAIR10.27.gtf.gzgzip -d Arabidopsis_thaliana.TAIR10.27.dna.genome.fa.gzgzip -d Arabidopsis_thaliana.TAIR10.27.gtf.gz# gtf文件格式转换gtfToGenePred -genePredExt Arabidopsis_thaliana.TAIR10.27.gtf AT_refGene.txt# wget -c http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/gtfToGenePred# 另一种格式转换方法，https://github.com/chengcz/pyGTF# 使用软件包提供脚本build物种数据库，数据库buildver为AT，名称为refGeneperl retrieve_seq_from_fasta.pl --format refGene --seqfile Arabidopsis_thaliana.TAIR10.27.dna.genome.fa AT_refGene.txt --out AT_refGeneMrna.fa]]></content>
      <categories>
        <category>bioinfor</category>
      </categories>
      <tags>
        <tag>vcf</tag>
        <tag>annot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从 vcf 文件准备 ANNOVAR 数据库]]></title>
    <url>%2F2018%2F201806_vcf2tab%2F</url>
    <content type="text"><![CDATA[annovar可以说是最常用的变异注释软件了，除了基于基因位置进行注释，还有丰富的第三方数据库支持，clinvar, cosmic等等，但是annovar提供下载数据库版本较老，需要自行下载第三方 vcf 进行转换。 以 clinvar 为例说说 vcf 格式转换成 annovar 可识别表格格式，annovar 仅需要 vcf 中部分信息，为了脚本通用这里输出vcf所有信息到 tab 格式，详细处理脚本如下，同时实现脚本存放 github。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273import gzipdef Parse_Variant_Annovar_Style(line): CHROM, POS, ID, REF, ALT, QUAL, FILTER, INFO = line.split('\t')[:8] # 一般公共数据下载的 vcf 文件仅包含8列 POS = int(POS) dat = &#123;&#125; dat['CHROM'] = CHROM dat['ID'] = ID if len(REF) == len(ALT) == 1 and REF != ALT: # SNV dat['START'], dat['END'] = POS, POS dat['REF'], dat['ALT'] = REF, ALT elif len(REF) == 1 and len(ALT) != 1 and ALT.startswith(REF): # Insert dat['START'], dat['END'] = POS, POS dat['REF'], dat['ALT'] = '-', ALT[1:] elif len(REF) != 1 and len(ALT) == 1 and REF.startswith(ALT): # Delete dat['START'], dat['END'] = POS+1, POS+len(REF)-1 dat['REF'], dat['ALT'] = REF[1:], '-' else: # complex variant dat['START'], dat['END'] = POS, POS+len(REF)-1 dat['REF'], dat['ALT'] = REF, ALT dat.update(INFO2dict(INFO)) return dat def Parse_Variant(line): CHROM, POS, ID, REF, ALT, QUAL, FILTER, INFO = line.split('\t')[:8] # 一般公共数据下载的 vcf 文件仅包含8列 POS = int(POS) dat = &#123;&#125; dat['CHROM'] = CHROM dat['ID'] = ID dat['START'], dat['END'] = POS, POS+len(REF)-1 dat['REF'], dat['ALT'] = REF, ALT dat.update(INFO2dict(INFO)) return dat def INFO2dict(info): tmp = [i.strip() for i in info.split(';') if i.strip()] infor = &#123;&#125; for i in tmp: if '=' in i: # index = i.index('=') # infor[i[:index]] = i[index+1:] i = i.split('=') infor[i[0]] = i[1] else: infor[i] = 'true' return inforfopen = gzip.open if clinvar.endswith('gz') else open# 这种文件打开适用于 python2, python3 中存在 bits 和 string 类型的差别with fopen(input_vcf) as f: tags = [] for i in f: if not i.strip(): continue if i.startswith('##INFO='): tags.append(i.split(',')[0].split('=')[-1]) elif i.startswith('#CHROM'): print('#CHROM\tSTART\tEND\tID\tREF\tALT\t&#123;&#125;\n'.format('\t'.join(tags))) elif not i.startswith('#'): dat = Parse_Variant_Annovar_Style(i) # dat = Parse_Varitn(i) for x, label in enumerate(['CHROM', 'START', 'END', 'ID', 'REF', 'ALT']+tags): if x != 0: print('\t') print(dat.get(label, '--')) # INFO 列中某些 tag 并不存在于所有行 print('\n')]]></content>
      <categories>
        <category>bioformat</category>
      </categories>
      <tags>
        <tag>vcf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编写 python 的几种 config 处理方式]]></title>
    <url>%2F2018%2F201805_config%2F</url>
    <content type="text"><![CDATA[编写分析流程时候，为了兼容性总会设置一些常用选项配置文件输入，不同配置方式各有一些特点，就用过的几种配置方式简单整理 script —— config.py123456import osimport sys# sys.path.insert(0, 'config_py_dirname') # 使config.py文件位于PYTHONPATH路径下import configoption = config.option # 获取配置信息 书写方式完全按照python脚本进行，可读性较高，问题在于PYTHONPATH的处理，脚本环境内键值对可能产生的覆盖问题，准确导入分析流程 configparser —— config.ini123456789101112import configparser # python3# import Configparser as configparser # python2cf = configparser.ConfigParser()cf.read('config.ini')sections = cf.sections() # 列出所有sectionoptions = cf.options(section) # 列出section对应的所有optionvalue = cf.get(section,option) # 范围section对应option的值，string# getint(section,option) 返回为int，# getboolean(section,option) 返回bool，# getfloat(section,option) 返回float 但是，在configparser模块中不区分section, option大小写，一次读取一条配置信息 json —— config.json123456789import json# 倒入配置json到python环境，config类型跟json内容相关with open('config.json') as f: config = json.load(f) # 输出python对象到json文件with open('whatever.josn', 'w') as f: json.dump(config, f, indent=2) json文件格式要求较严格，书写配置文件容易手误出错，相比之下更适用于流程中数据传递 yaml —— config.yaml1234import yamlwith open('config.yaml') as f: config = yaml.load(f) # dict OR list YAML 语法规则 大小写敏感 使用缩进表示层级关系 使用空格缩进,不允许使用Tab键 相同层级的元素左侧对齐即可 使用 # 作为注释符号 123456789101112131415161718# 表示 key-value 对key1: value2key2: value2# 表示 list- value1- value2# [value1, value2]# 时间表示date: 2018-05-24 # iso8601# None表示key: ~# bool表示flag1: trueflag2: false 相比于上面几种方法，yaml可读性高，容易书写，不用考虑PYTHONPATH问题，批量导入配置信息]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>config</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HGVS 变异描述规则]]></title>
    <url>%2F2018%2F201805_HGVS%2F</url>
    <content type="text"><![CDATA[http://varnomen.hgvs.org/recommendations/, 描述规则基本格式为：[identifier]:[levels].[Description of variant] [identifier] only public files from NCBI or EBI are accepted as reference sequence files a reference sequence file identifier should contain both the accession and version number specifications to a specific annotated segment of a reference sequence can be given in parentheses directly after the reference sequence the reference sequence used must contain the variant residue described 其他具体要求描述可以参考：http://varnomen.hgvs.org/bg-material/refseq/ [levels] Reference Sequence Types主要有g, c, n, m, r, p六种，其中c, p, g使用最多 [Description of variant] 替换 cDNA: c.255C&gt;A protein: p.Trp24Cys, p.Trp24*, p.Cys188= 缺失 cDNA: c.19_21del protein: p.Lys23_Val25del 插入 cDNA: c.240_241insAGG protein: p.Lys2_Gly3insGlnSerLys 重复 cDNA: c.20_23dup protein: p.Ser6dup 插入缺失 cDNA: c.142_144delinsTGG protein: p.Arg48Trp 移码突变 protein: p.Arg97fs Notes 坐标系统为 1-based, 描述核酸和蛋白变异的差别，g.123A&gt;G 和 p.Trp26Ter 可使用在线网站 mutalyzer 检验变异描述是否合规，也可进行c -&gt; g坐标转换等等 ###Ref: DOI: 10.1002/humu.22981 http://varnomen.hgvs.org/recommendations/general]]></content>
      <categories>
        <category>standard</category>
      </categories>
      <tags>
        <tag>HGVS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python 表格处理 —— pandas]]></title>
    <url>%2F2018%2F201805_pandas%2F</url>
    <content type="text"><![CDATA[pandas 表格数据处理，唰唰唰… Data frame12345678910111213141516171819202122232425262728293031323334353637import pandas as pd# 数据框转制dat = dat.T# 根据列进行合并dat = pd.merge(dat1, dat2, on='col', # col在两个数据框中同时存在时，设置on left_on='col1', right_on='col2', # 两个列名不同时存在，分贝设置合并列名 left_index=False, right_index=False, # 使用df的index进行合并 how='outdir') # &#123;'left', 'right', 'outer', 'inner'&#125;# concat 合并列数据dat = pd.concat(dflst, axis=1, # 左右合并 join='outer') # &#123;'inner', 'outer'&#125;# concat 行追加dat = pd.concat(dflst, axis=0, ignore_index=True)# 删除数据框行或者列dat = dat.drop(col_lst, axis=1) # 删除 列dat = dat.drop(index_lst, axis=0) # 删除 行dat = dat.drop_duplicates() # 删除 重复行dat = dat.dropna(axis=0, # 0，1 0表示行，1表示列 how='all') # &#123;'any', 'all'&#125;, all表示删除所有值缺失，any表示删除任意一个值缺失# 长表格 ==&gt; 宽表格转换import numpy as npdat = pd.pivot_table(df, index=["Product"], # 设置保留字段 columns=['Year'], # 扩展列 values=["Price"], # 选定列对应数值 aggfunc=[np.sum], fill_value=0) # 设置数据合并的具体操作# 宽表格 ==&gt; 长表格转换df = df.melt(id_vars=["Name","Product"], # 保留的主字段 var_name="Year", # 分类名 value_name="Price") # 变量名 read table123456789101112131415161718192021222324252627import pandas as pddat = pd.read_table('data.csv', sep='\t', # 设置表格分隔符，read_table默认为‘\t’，read_csv默认为‘,’ index_col=0, # 使用第1列作为数据框index，默认range(0, num_line)作为index header=None, # 表示文件不存在表头，默认第一行为表头 names=['a', 'b', '']) # 表头不存在时，添加表头为...# dat = pd.read_csv(...)dat = pd.read_excel('data.xlsx', sheet_name='sheet1') # 读取data.xlsx工作簿中sheet1的表格# 读取文件前特定行数dat = pd.read_csv('data.csv', nrows=500) # 读取超大文件reader = pd.read_csv('data.csv', iterator=True) # 产生一个迭代器chunkSize = 100000chunks = []while True: try: chunk = reader.get_chunk(chunkSize) # 一次读取特定行 # chunk # 进行一些过滤、处理 chunks.append(chunk) except StopIteration: breakdf = pd.concat(chunks, axis=0, ignore_index=True) save table123456789101112131415161718import pandas as pd# 输出数据到txt文本dat.to_csv('data.xls', index=False, # 是否输出df的index header=False, # 是否输出表头 na_rep='', # 缺省值替换 sep='\t', # 使用tab作为分隔符 encoding='utf-8') # 输出文件使用utf-8编码# 输出数据框到excelwriter = pd.ExcelWriter('output.xlsx') # 创建输出文件df1.to_excel(writer, sheet_name='Sheet1', ) # 输出到工作簿中名为Sheet1的工作表df2.to_excel(writer, sheet_name='Sheet2', ) # 输出到工作簿中名为Sheet2的工作表# ... writer.save() # 保存工作簿]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python 参数解析 —— argsparse]]></title>
    <url>%2F2018%2F201805_argparse%2F</url>
    <content type="text"><![CDATA[argsparse 解析命令行参数，一些常用参数整理。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import argparseparser = argparse.ArgumentParser(# prog=__file__, # 设置项目名称，__file__ 与 sys.argv[0] 一致，默认显示为 basename(__file__) usage=''' python %(prog)s [option] ''', # 设置脚本输出默认 USAGE 信息 description=''' some script description ''', # 脚本功能描述等等 formatter_class=argparse.RawTextHelpFormatter, # 以脚本内原始文档格式输出描述等信息 epilog=''' Note and input example, and so on. ''') # 脚本描述以外的其他信息parser.add_argument('-v', '--version', # 添加脚本版本信息参数 action='version', version='%(prog)s 2.0')# 添加一个普通的可选参数parser.add_argument('-e', # 短参数 '--example', # 长参数 dest='example', # 参数对应模块内部，唯一标识符 metavar='', # 设置help信息中参数后字符 action='store', # 参数处理方式 type=str, # 传入参数类型 choices=['a', 'b', 'c'], # 参数选择范围 required=True, # 表示该参数必须被设置 default='a', # 参数缺省时，该参数默认值 nargs='+', # 参数个数，*+？，表示至少一个输入 help='some help infor') # help信息# action属性为store_true, store_false, store_const时，参数不接任何输入信息parser.add_argument('--sum', dest='accumulate', action='store_const', # 该参数被设置时，参数取值成 const 对应值 const=sum, default=max, help='sum the integers (default: find the max)')# 位置参数parser.add_argument('pos', dest='pos_argument', metavar='', nargs='+', # 参数输入文件个数 help='some help infor')# 互斥参数组，组内参数不可同时出现，required=True表示组内必须有参数被设置exclusive = parser.add_mutually_exclusive_group(required=True) exclusive.add_argument('-a', )exclusive.add_argument('-b', )# 设置多个参数为一组，无实际意义，仅在打印help信息时，分组显示group = parser.add_argument_group('argument group info') group.add_argument('-c', )group.add_argument('-d', )# parse.print_help()args = parser.parse_args() # 参数解析，生成可调用对象example = args.example# ...]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>argparse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python 执行 linux 命令 —— subprocess]]></title>
    <url>%2F2018%2F201805_subprocess%2F</url>
    <content type="text"><![CDATA[python 脚本内执行 linux 命令有多种方式，os.system(), os.popen()也可以，官方最新推荐方式为subprocess 123456789101112131415161718192021222324import subprocess# 执行cmd并等待运行结束，返回执行状态码subprocess.call('ls', shell=True) # 与 os.system() 功能相似# 执行cmd并等待运行结束，返回执行状态码subprocess.check_call('ls', shell=True) # 与subprocess.call()不同，returncode不为0，触发CalledProcessError异常# 执行cmd并等待运行结束，返回 /dev/stdout 信息subprocess.check_output('ls', shell=True, universal_newlines=True) # 以 string 形式返回 stdout 信息，而不是bytes，与 os.popen() 功能相似# 不等待 cmd 运行结束job = subprocess.Popen('ls -l', # string of cmd shell=True, # 传入参数作为 shell 进行执行 stdout=subprocess.PIPE, # 输出标准输出到subprocess流程 stderr=subprocess.PIPE, # 输出标准错误到subprocess流程 universal_newlines=True) # stdout 返回时，使用 string 代替 bytes 类型pid = job.pid # 获得 cmd 本地执行的 pidjob.wait() # 等待工作结束，默认Popen不等待 cmd 执行完毕即退出stdout, stderr = job.communicate() # 获得 cmd 执行返回标准输出、标准错误# 设置 stdin=subprocess.PIPE 时，可以在communicate(input='...')传入参数job.returncode # 获得 cmd 执行状态码job.kill() # 杀死 cmd 进程 os 模块实现 linux cmd 执行 12345import osos.system('ls -l') # 直接返回 cmd 执行状态码# assert not os.system('ls')os.popen('ls -l') # 以文件句柄的方式返回 cmd 执行的stdout信息]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>suprocess</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 topaht2-fusion 分析 Fusion-Gene]]></title>
    <url>%2F2018%2F201805_TophatFusion%2F</url>
    <content type="text"><![CDATA[tophat2-fusion 文档搬运工：https://ccb.jhu.edu/software/tophat/fusion_tutorial.shtml ###step1.tophat 1tophat2 -p 8 -o $OUTPUT --fusion-search --keep-fasta-order --bowtie1 --no-coverage-search --fusion-anchor-length 10 --fusion-min-dist 100000 --mate-inner-dist 40 --mate-std-dev 100 $hg_bowtie1_index $read1.fastq.gz $read2.fastq.gz Note: –fusion-search，融合 reads 搜索 –keep-fasta-order, –bowtie1，使用 bowtie1 代替 bowtie2 进行 reads 比对。需要使用 bowtie-1.1.2，最新版本 1.2.2 会报错 –no-coverage-search, –fusion-anchor-length 10 , –fusion-min-dist 100000 , –mate-inner-dist 40 , –mate-std-dev 100 , ###step2.tophat-fusion-post 1tophat-fusion-post -p 8 --num-fusion-reads 1 --num-fusion-pairs 2 --num-fusion-both 5 $hg_bowtie1_index Note: –num-fusion-reads* , 支持融合的最少 reads 数目 –num-fusion-pairs, 支持融合事件的最少配对 reads 数，表示为跨越融合点的测序片段 –num-fusion-both, 支持融合的最少 reads，包括跨越的 reads 对和 split reads –fusion-read-mismatches, Reads support fusions if they map across fusion with at most this many mismatches. The default is 2. –fusion-multireads, Reads that map to more than this many places will be ignored. The default is 2. –non-human, If your annotation is different from that of human, use the option. –fusion-pair-dist, Pairs with this specified distance are counted as supporting evidence for the fusion. The default threshold for the inner distance is 250. results测试了多个样品，多个参数，也根据官网数据、命令运行项目，都没有发现任何融合结果，bug还是打开方式不对]]></content>
      <categories>
        <category>bioinfor</category>
      </categories>
      <tags>
        <tag>FusionGene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 STAR-fusion 分析 Fusion-Gene]]></title>
    <url>%2F2018%2F201805_STARFusion%2F</url>
    <content type="text"><![CDATA[软件文档：https://github.com/STAR-Fusion/STAR-Fusion/wiki ##软件安装 STAR，https://github.com/alexdobin/STAR 运行依赖 perl 模块 DB_File URI::Escape Set::IntervalTree Carp::Assert JSON::XS PerlIO::gzip common::sense Types::Serialiser Canary::Stability ##数据库准备 下载一个较小的未处理的参考文件，自己运行 index 命令。要是网速够快也可以直接在 index 好的数据库文件，~27G 1234567wget -c https://data.broadinstitute.org/Trinity/CTAT_RESOURCE_LIB/GRCh37_v19_CTAT_lib_Feb092018.source_data.tar.gz$STAR_FUSION_HOME/FusionFilter/prep_genome_lib.pl \ --genome_fa ref_genome.fa \ --gtf ref_annot.gtf \ --fusion_annot_lib CTAT_HumanFusionLib.v0.1.0.dat.gz \ --annot_filter_rule AnnotFilterRule.pm \ --pfam_db PFAM.domtblout.dat.gz ##运行STAR-Fusion STAR-Fusion 对 STAR 输出的嵌合比对分析发现可能存在的基因融合事件 从 fastq 文件开始123456$STAR_FUSION_HOME/STAR-Fusion \ --genome_lib_dir /path/to/your/CTAT_resource_lib \ --left_fq reads_1.fq \ --right_fq reads_2.fq \ --output_dir star_fusion_outdir \ --no_remove_dups 从 STAR 产生 Bam 文件开始1234567891011121314151617STAR --genomeDir $&#123;star_index_dir&#125; \ --readFilesIn $&#123;left_fq_filename&#125; $&#123;right_fq_filename&#125; \ --twopassMode Basic \ --outReadsUnmapped None \ --chimSegmentMin 12 \ --chimJunctionOverhangMin 12 \ --alignSJDBoverhangMin 10 \ --alignMatesGapMax 100000 \ --alignIntronMax 100000 \ --chimSegmentReadGapMax 3 \ --alignSJstitchMismatchNmax 5 -1 5 5 \ --runThreadN $&#123;THREAD_COUNT&#125; \ --outSAMstrandField intronMotifSTAR-Fusion --genome_lib_dir /path/to/your/CTAT_resource_lib \ -J Chimeric.out.junction \ --output_dir star_fusion_outdir 输出文件STAR 速度还是那么让人惊喜，6m reads不到半小时。 融合结果star-fusion.fusion_predictions.abridged.tsv FusionName,JunctionReadCount, split align到融合点的序列片段数SpanningFragCount, 双端reads跨越融合点的序列片段数SpliceType, 断点是否在注释文件存在LeftGene,LeftBreakpoint,RightGene,RightBreakpoint,LargeAnchorSupport,FFPM, fusion fragments per million total readsLeftBreakDinuc,LeftBreakEntropy,RightBreakDinuc,RightBreakEntropy,annots, 结果比较真是一个悲伤的故事，根据官网给的两种运行方式，结果差别这么大。查看 STAR-Fusion 脚本，使用的 mapping 参数差异有点大啊，哪一个比较合理呢（一个新坑）？？？]]></content>
      <categories>
        <category>bioinfor</category>
      </categories>
      <tags>
        <tag>FusionGene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 trimmomatic 进行数据质控]]></title>
    <url>%2F2018%2F201805_trimmomatic%2F</url>
    <content type="text"><![CDATA[trimmomatic 使用 java 编写，免安装多平台运行，同时运行速度非常快。 1234567891011# paired end java -jar trimmomatic-0.32.jar PE -threads 8 -phred33 \ sample_R1.fastq.gz sample_R2.fastq.gz \ sample_R1_paired.fastq.gz sample_R1_unpaired.fastq.gz \ sample_R2_paired.fastq.gz sample_R2_unpaired.fastq.gz \ ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 SLIDINGWINDOW:4:15 MINLEN:36# single endjava -jar trimmomatic-0.32.jar PE -threads 8 -phred33 \ sample.fastq.gz sample_clean.fastq.gz \ ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 SLIDINGWINDOW:4:15 MINLEN:36 参数选择PE/SE 设定对 Paired-End 或 Single-End 的 reads 进行处理，其输入和输出参数稍有不一样 threads 设置多线程运行数 phred33/phred64 设置碱基的质量格式，可选 phred64 ILLUMINACLIP:::::: 切除 adapter 序列。参数后分别接 adapter 序列的 fasta 文件，允许的最大 mismatch 数， palindrome 模式下匹配碱基数阈值： simple 模式下的匹配碱基数阈值 minAdapterLength：只对 PE 测序的 palindrome clip 模式有效，指定 palindrome 模式下可以切除的接头序列最短长度，由于历史的原因，默认值是 8，但实际上 palindrome 模式可以切除短至 1bp 的接头污染，所以可以设置为 1 。keepBothReads：只对 PE 测序的 palindrome clip 模式有效，这个参数很重要，在上图中 D 模式下， R1 和 R2 在去除了接头序列之后剩余的部分是完全反向互补的，默认参数 false，意味着整条去除与 R1 完全反向互补的 R2，当做重复去除掉，但在有些情况下，例如需要用到 paired reads 的 bowtie2 流程，就要将这个参数改为 true，否则会损失一部分 paired reads。 SLIDINGWINDOW 从 reads 首端（ 5’端）开始进行滑动，当滑动位点周围一段序列(window)的平均碱基低于阈值，则从该处进行切除。 Windows 的 size 是 4 bp， 若其平均碱基质量小于15，则切除 MAXINFO：LEADING/TRAILING&gt; 切除 reads 首端（ 5’端） / reads 末端（ 3’端）碱基质量小于指定值的碱基 CROP/HEADCROP 从 reads 末端（ 3’端）/reads 首端（ 5’端）切除碱基到指定长度 MINLEN 抛弃低于指定长度的 reads TOPHRED33/TOPHRED64 转换碱基质量格式，Illumina HiSeq 2000质量系统为phred-64，可用该参数转换到phred-33]]></content>
      <categories>
        <category>bioinfor</category>
      </categories>
      <tags>
        <tag>QC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VCF 注释 —— SnpEff]]></title>
    <url>%2F2018%2F201805_snpEff%2F</url>
    <content type="text"><![CDATA[日常工作之读文档，了解参数改变、新方法功能，对已有流程进行修改。 各种文档看一边遍一遍，使用需要又一次看 SnpEff （http://snpeff.sourceforge.net/index.html ），进行一些整理如下，不过还是建议原文，鬼知道我会在哪里写错理解错。 运行 annotation 12345678java -jar snpEff.jar eff genome input.vcf &gt;output.vcf# -hgvs, 默认开启，使用 HGVS 变异描述语法对蛋白替换进行描述# -fi, 提供bed文件，只对bed区域内位点进行注释# -no-downstream, -no-intergenic, -no-intron, -no-upstream, -no-utr, -no [effectType], 参数还可对根据注释类型对注释进行初过滤# -canon, 只使用基因对应最长转录本（作为权威转录本）进行注释；参数-canonList可以自行提供基因权威转录本，格式为“GeneID transcript_ID”# -noStats, 不输出注释统计信息# -v, 话痨模式，输出各种运行信息 ​ database 人、小鼠等物种都有已经 build 的数据库，可以直接下载，链接：https://sourceforge.net/projects/snpeff/files/databases/v4_3/，注释选择基因组版本。 1234java -jar snpEff.jar databases # 查看可用数据库java -jar snpEff.jar download -v GRCh37.75 # 下载数据库 GRCh37.75# Q, 使用执行程序下载数据库时容易断，一般建议手动下载，或者wget等其他方式 软件还提供了自行构建数据库，详细用法如下： 12345678910111213141516171819# 添加配置信息echo 'GRCh37.70.genome : Human ' &gt;&gt;~/snpEff/snpEff.config# Create directoy for this new genomemkdir -p ~/snpEff/data/GRCh37.70 &amp;&amp; cd ~/snpEff/data/GRCh37.70# Get annotation fileswget -c -O genes.gtf.gz ftp://ftp.ensembl.org/pub/release-70/gtf/homo_sapiens/Homo_sapiens.GRCh37.70.gtf.gz# Get the genomewget -c -O sequences.fa.gz ftp://ftp.ensembl.org/pub/release-70/fasta/homo_sapiens/dna/Homo_sapiens.GRCh37.70.dna.toplevel.fa.gz# Optional, Download CDSswget -c -O cds.fa.gz ftp://ftp.ensembl.org/pub/release-70/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh37.70.cdna.all.fa.gz# Optional, Download proteinswget -c -O protein.fa.gz ftp://ftp.ensembl.org/pub/release-70/fasta/homo_sapiens/pep/Homo_sapiens.GRCh37.70.pep.all.fa.gz# Optional, Download regulatory annotationswget -c -O regulation.gff.gz ftp://ftp.ensembl.org/pub/release-70/regulation/homo_sapiens/AnnotatedFeatures.gff.gz# Building a database from GFF filescd ~/snpEff &amp;&amp; java -jar snpEff.jar build -gff3 -v GRCh37.70# rm -r ~/snpEff/data/GRCh37.70 此外，snpEff 构建数据库还支持 RefSeq, GenBank 等多种数据输入格式，具体用法查看原文档。 注意物种 codon 选择，codon 可以在 https://www.ncbi.nlm.nih.gov/Taxonomy/Utils/wprintgc.cgi 中查看，codon可以在~/snpEff/snpEff.config文件中设置： 1234# 设置特异 codoncodon.Invertebrate_Mitochondrial: TTT/F, TTC/F, TAC/Y, TAA/*, ATG/M+, ATG/M+, ACT/T, ...# 物种选择恰当的 codontabledm3.M.codonTable : Invertebrate_Mitochondrial # the chromosome &apos;M&apos; from fly genome (dm3) uses Invertebrate_Mitochondrial codon table ​ dump 从数据库中提取出注释信息 123java -jar snpEff.jar dump -v -bed GRCh37.70 &gt; GRCh37.70.bed# [-bed |-txt ] 设置数据格式# [-0 |-1 ] 设置坐标系统，0-based 或者 1-based 注释结果 可以看到注释信息被添加到了 VCF 中每个变异 INFO 信息中，以 ANN= 特征开始，详细的注释说明可查看官方文档： http://snpeff.sourceforge.net/VCFannotationformat_v1.0.pdf。因为基因多个转录本、相互重叠基因等原因，可以看到变异位点被多次注释，需根据实际需求进行筛选。]]></content>
      <categories>
        <category>bioinfor</category>
      </categories>
      <tags>
        <tag>vcf</tag>
        <tag>annot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卸载一个假忙的硬盘]]></title>
    <url>%2F2017%2F201709_umount%2F</url>
    <content type="text"><![CDATA[df -h查看卸载硬盘信息 umount /dev/sdf，停止使用的硬盘假装忙碌状态 123umount: /mnt/usbhd2: device is busy. (In some cases useful info about processes that use the device is found by lsof(8) or fuser(1)) fuser /dev/sdf，查看占用硬盘的进程 1/mnt/sdf: 106657c kill -9 106657 重新执行umount /dev/sdf]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[shell 命令使用笔记]]></title>
    <url>%2F2017%2F201709_ShellNote%2F</url>
    <content type="text"><![CDATA[&amp;&amp;，cmd1 &amp;&amp; cmd2 如果cmd1成功执行（返回0，注意设置中脚本执行返回值），那么执行cmd2 ​ ||，cmd1 || cmd2 如果cmd1执行失败，那么执行cmd2 ​ (), {}组合逻辑控制 ()组合来控制命令； {}组合控制子shell ​ 条件判断 1234567891011121314151617181920212223242526if [ condition ]; then# if test condition; then echo 'yes'fi#使用[]判断条件时，[]和condition之间必须用空格分隔开# -d 判断是不是文件夹# -f 判断是不是文件# -L 判断是不是链接# -s 判断文件是否存在且非空# -r 判断文件是否可读# -w 判断文件是否可写# -x 判断文件是否可执行# -z 判断是否空字符串# -n 判断是否非空字符串# = 判断字符串是否相等# != 判断字符串是否不等# -eq 判断数值是否相等# -ne 判断数值是否不等# -gt 第一个数大于第二个数# -ge 第一个数大于等于第二个数# -lt 第一个数小于第二个数# -le 第一个数小于等于第二个数# -a 逻辑与# -o 逻辑或# ! 逻辑否 ​ 推出状态 exit n，其中n表示数字，0表示脚本执行成功无错误，1表示执行失败某处有错误 可以使用 $? 获得最后执行命令推出状态 ​ shell脚本 [ ] $# 获取脚本输出参数长度 [ ] $@ 获取脚本所有参数 循环 1234for i in `ls ` do echo 'yes' done ​ 序列生成 123seq 3 # 1 2 3seq 3 5 # 3 4 5seq 5 2 10 # 5 7 9 ​ 随机数生成 1234567echo $RANDOM# RANDOM是bash的一个内建函数，会返回一个[0, 32676]内的整数# 生成一定范围内的随机数beg=5end=15echo $((RANDOM % ($end - $beg) + $beg)) ​ shell 数学计算 123echo $((9-4)) # 5echo $((9/4)) # 2echo $((9/-4)) # -2 ​]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[提交测序数据到 SRA]]></title>
    <url>%2F2017%2F201709_submitDataToSRA%2F</url>
    <content type="text"><![CDATA[九月一号开学了，暑假作业写完了没有？但是跟我并没有关系，不开学，没作业，但是好多事情啊，，，最近提交了一些数据到SRA，每次过程总是那么曲折离奇，对一些遇到的问题进行整理填坑吧。 简单说，申请BioProject，申请BioSample，填写SRA_metadata，上传数据，虽然过程清晰，坑还没填完，仍在邮件NCBI，过程供参考。 1. BioProject申请一个项目编号，填写submitter 信息，选择项目类型（Raw sequence reads），物种名称，释放时间，项目描述，BioSample和文章信息（跳过，跳过，，），核对提交 2. BioSample申请一个样品编号，submitter 信息，释放时间，样品类型，填写样品属性表格，核对提交 3. SRA主要就填写SRA_metadata，测序平台，文库类型等等，选择批量提交，填写表格注意看要求啊，除了样品名称、登录号等信息，其他属性组合每行也要是唯一的。 4. submit data看数据大小，选择合适上传方法吧，小于2G用aspera浏览器插件，大数据选择ftp或者aspera命令行。这几次都用了aspera cmd上传，霸占几乎全部网速，很快上传好 具体命令为：ascp -i &lt;path/to/key_file&gt; -QT -l100m -k1 -d &lt;path/to/folder/containing files&gt; subasp@upload.ncbi.nlm.nih.gov:uploads/XXX@xxx.com_g3O1FgOE， 其中key必须是全路径，-d接包括原始数据的文件夹（不再包含其他文件夹），XXX@xxx.com为注册账号邮箱 上传结束后，SRA中选择上传数据文件夹，，， 好了，装逼结束，文件没仔细检查，传了两不完整的fastq，发邮件给ncbi沟通呢，md5值也还没有用到，不知什么情况，，，too young, too naive…]]></content>
      <categories>
        <category>bioinfor</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python class 实现 fasta 文件解析]]></title>
    <url>%2F2017%2F201708_object%2F</url>
    <content type="text"><![CDATA[没有对象怎么办（object）？？？当然是new一个啊 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class Sequence(object): # 使用__init__对Sequence进行初始化，Sequence对象具有三个属性 def __init__(self, name, seq, descr=''): self.name = name self.seq = seq self.descr = descr # 给Sequence对象定义一个反向互补方法 def reverse_complement(self): RC = &#123;'A':'T','T':'A','C':'G','G':'C','N':'N', 'a':'t','t':'a','c':'g','g':'c','n':'n'&#125; return Sequence(self.name, ''.join([RC[i] for i in self.seq[::-1]]), self.descr) # 定义输出到文件的方法，接受文件句柄输入 def write_to_fasta_file(self, file_handle): def _SeqFormat(seq, chara=80): tmp = '' for i in range(0,len(seq),chara): tmp += (seq[i:(i+chara)]+'\n') return tmp file_handle.write('&gt;&#123;&#125; &#123;&#125;\n'.format(self.name, self.descr)) file_handle.write('&#123;&#125;'.format(_SeqFormat(self.seq)))class FastaReader(object): def __init__(self, fastafile): self.fasta = fastafile # 使用__iter__方法实现对fasta文件的循环解析，其中用到yeild构造了生成器 def __iter__(self): with open(self.fasta) as f: seq = None for line in f: if line.startswith( "&gt;" ): if seq: yield Sequence( name, seq, descr ) name, sep, descr = line[1:-1].partition(' ') seq = "" else: assert seq is not None, "FASTA file does not start with '&gt;'." seq += line[:-1].encode() if seq is not None: yield Sequence( name, seq, descr )# examplefor i in FastaReader('ref.fa'): # 使用 i.write_to_fasta_file(file_handle) i.seq i.name i.descr 懒癌发作了，就这，，，]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>fasta</tag>
        <tag>object</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器挂载 4T 硬盘]]></title>
    <url>%2F2017%2F201707_mountHardDisk%2F</url>
    <content type="text"><![CDATA[因为一些我并不理解的原因，服务器不能正确识别4T的NTFS移动硬盘，需要对其格式化成ext4格式，就我的一些操作记录。 fdisk -l，查看硬盘分配的硬件编号 parted /dev/sdg，对使用parted工具对硬盘进行分区 mklable gpt，把磁盘格式化为 gpt分区表 mkpart primary 0 4000G，创建一个主分区，起始位置-结束位置 mkpart extended 2000G 4000G，可选，创建一个扩展分区 print，打印出当前设备的信息 quit，推出parted mkfs.ext4 /dev/sdg1，对硬盘进行格式化，如果存在多个分区，每个分区都需要进行格式化 mount /dev/sdg1 /mnt/usbmount1，挂载硬盘到服务器 umount /dev/sdg1，硬盘卸载]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[一次诡异的 python 错误排查]]></title>
    <url>%2F2017%2F201707_VariableName%2F</url>
    <content type="text"><![CDATA[输入表格示例： library reads_num reads_len bases_num GC avgQ Q20 Q30 A-Ctrl 46,875,170 150.0 7.00 Gb 49.90% 36.6 93.24% 85.97% B-Ctrl 154,351,882 150.0 23.00 Gb 45.40% 36.9 93.80% 86.93% C-Ctrl 58,126,912 150.0 8.00 Gb 60.05% 36.05 92.19% 83.96% D-Ctrl 143,761,494 150.0 21.00 Gb 42.00% 37.25 94.46% 88.15% 初次尝试12345678# 错误的结果table = open('qc.stat', 'r')header = [[],] * len(table.readline().strip().split('\t'))for i in table: for x,y in enumerate(i.strip().split('\t')): header[x].append(y)header# table.close() 结果输出[[&apos;A&apos;, &apos;46,875,170&apos;, &apos;150.0&apos;, &apos;7.00 Gb&apos;, &apos;49.90%&apos;, &apos;36.6&apos;, &apos;93.24%&apos;, &apos;85.97%&apos;, &apos;B&apos;, &apos;154,351,882&apos;, &apos;150.0&apos;, &apos;23.00 Gb&apos;, &apos;45.40%&apos;, &apos;36.9&apos;, &apos;93.80%&apos;, &apos;86.93%&apos;, &apos;C&apos;, &apos;58,126,912&apos;, &apos;150.0&apos;, &apos;8.00 Gb&apos;, &apos;60.05%&apos;, &apos;36.05&apos;, &apos;92.19%&apos;, &apos;83.96%&apos;, &apos;D&apos;, &apos;143,761,494&apos;, &apos;150.0&apos;, &apos;21.00 Gb&apos;, &apos;42.00%&apos;, &apos;37.25&apos;, &apos;94.46%&apos;, &apos;88.15%&apos;], [&apos;A&apos;, &apos;46,875,170&apos;, &apos;150.0&apos;, &apos;7.00 Gb&apos;, &apos;49.90%&apos;, &apos;36.6&apos;, &apos;93.24%&apos;, &apos;85.97%&apos;, &apos;B&apos;, &apos;154,351,882&apos;, &apos;150.0&apos;, &apos;23.00 Gb&apos;, &apos;45.40%&apos;, &apos;36.9&apos;, &apos;93.80%&apos;, &apos;86.93%&apos;, &apos;C&apos;, &apos;58,126,912&apos;, &apos;150.0&apos;, &apos;8.00 Gb&apos;, &apos;60.05%&apos;, &apos;36.05&apos;, &apos;92.19%&apos;, &apos;83.96%&apos;, &apos;D&apos;, &apos;143,761,494&apos;, &apos;150.0&apos;, &apos;21.00 Gb&apos;, &apos;42.00%&apos;, &apos;37.25&apos;, &apos;94.46%&apos;, &apos;88.15%&apos;], ...] 正确解析12345678# 正确代码table = open('qc.stat', 'r')header = [[i,] for i in table.readline().strip().split('\t')]for i in table: for x,y in enumerate(i.strip().split('\t')): header[x].append(y)header# table.close() 结果输出[[&apos;library&apos;, &apos;A&apos;, &apos;B&apos;, &apos;C&apos;, &apos;D&apos;], [&apos;reads_num&apos;, &apos;46,875,170&apos;, &apos;154,351,882&apos;, &apos;58,126,912&apos;, &apos;143,761,494&apos;], [&apos;reads_len&apos;, &apos;150.0&apos;, &apos;150.0&apos;, &apos;150.0&apos;, &apos;150.0&apos;], [&apos;bases_num&apos;, &apos;7.00 Gb&apos;, &apos;23.00 Gb&apos;, &apos;8.00 Gb&apos;, &apos;21.00 Gb&apos;], [&apos;GC&apos;, &apos;49.90%&apos;, &apos;45.40%&apos;, &apos;60.05%&apos;, &apos;42.00%&apos;], [&apos;avgQ&apos;, &apos;36.6&apos;, &apos;36.9&apos;, &apos;36.05&apos;, &apos;37.25&apos;], [&apos;Q20&apos;, &apos;93.24%&apos;, &apos;93.80%&apos;, &apos;92.19%&apos;, &apos;94.46%&apos;], [&apos;Q30&apos;, &apos;85.97%&apos;, &apos;86.93%&apos;, &apos;83.96%&apos;, &apos;88.15%&apos;]] 原因分析12345678910111213# 错误原因查找head = '#library\treads_num\treads_len\tbases_num\tGC\tavgQ\tQ20\tQ30'header = [[], ] * len(head.strip().split('\t'))print 'error method...'for i in header: print id(i)header = [[i,] for i in head.strip().split('\t')]print '\nright method...'for i in header: print id(i) error method... 4435237272 4435237272 4435237272 4435237272 4435237272 4435237272 4435237272 4435237272 right method... 4435236984 4435354904 4435318688 4435237416 4435318040 4435319768 4435318472 4435319624 python独特的变量命名方式：变量名（&gt;= 1个）指向储存数据物理地址，使用其中任何一个名称都可以对数据进行操作]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python 字典]]></title>
    <url>%2F2017%2F201706_pythondict%2F</url>
    <content type="text"><![CDATA[字典也是哈希表，键(key)必须是唯一的可哈希对象，但值(value)则不必。python中键值间使用“:”分割，不同键值对间使用“,”分割，整个字典包括在“{}”中。 123#初始化定义seq = &#123;&#125; #seq = dict()seq = &#123;'key1':'value1', 'key2':'value2'&#125; 设置字典默认值123456789#key不存在时，返回自定义valueseq.get(key, value)#key不存在时，设置key对应键值为list；key存在时返回seq[key]seq.setdefault(key,[])#设置默认字典，读取seq[key]时进行初始化from collections import defaultdictseq = defaultdict(list) 树结构 1234567from collections import defaultdictdef tree(): return defaultdict(tree)users = tree()users['codingpy']['username'] = 'earlgrey'#甚至还能不赋值users['Python']['Standard Library']['os'] 计算list中某元素出现次数123456# 方法一list.count('X') # int# 方法二from collections import CounterCounter(list)['X'] # dict['X'], int]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>hashmap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 命令 —— find]]></title>
    <url>%2F2017%2F201705_linuxfind%2F</url>
    <content type="text"><![CDATA[find，Linux常用文件搜索命令简要整理 基本用法find基本用法如：find [path] &lt;search regular&gt; [action] 可选参数path，支持空格分隔的多路径查找，默认为当前路径 可选参数action -print，匹配文件输出到stdout，默认 -exec，对find匹配到的文件执行该参数所给出的shell命令，格式为command { } \;，其中{}表示匹配文件 -ok，于-exec类似，执行前给出提示是否执行 文件名 123find [path] -name &lt;name&gt;find [path] -iname &lt;name&gt; #不区分大小写# 其中&lt;name&gt;支持通配符，一般使用时建议增加单引号 文件类型，一般配合其他搜索参数一起使用 12find [path] -type [bdcplf]# 其中d表示目录，l表示链接，f表示文件 文件大小 123# -size 参数支持+-表示大于小于，ckMG表示数据单位find [path] -size +10M #path下大于10M的文件find [path] -size -1G #path下小于1G的文件 时间戳 1234find [path] -mtime -n +m #更改时间为m天前n天内的文件find [path] -atime -n +m #访问时间find [path] -ctime -n +m #创建时间# mtime参数后数字表示天， mmin参数后数字表示分钟 文件属主 1234find [path] -user &lt;user_name&gt; #path下属主为user_name的文件find [path] -group &lt;group_name&gt; #path下组名为group_name的文件find [path] -nouser #用户ID不存在的文件find [path] -nogroup #组ID不存在的文件 文件权限 1234find [path] -perm 755 #权限设置为755的文件find [path] -perm -u=r #文件属主有读权限的目录或文件find [path] -perm -g=r #用户组有读权限的目录或文件find [path] -perm -o=r #其它用户有读权限的目录或文件 其他搜索参数 1234find [path] &lt;search_regular&gt; [option]# -follow 追踪链接文件# -mount 不跨越文件系统mount点# -empty 空文件 示例 多筛选条件组合 1234find [path] -name '*R' -a -mtime -1 #修改时间一天内的R脚本文件#-a | -and # 同时满足#-o | -or # 或#-not # 条件取反 几个例子，来自微博@linux命令行精选网 123456find . ! -name &lt;NAME&gt; -delete #用find删除文件时候排除特定文件find . -type l -xtype l #查找失效的符号链接find . -iname '*.jpg' | sed 's/.*/&lt;img src="&amp;"&gt;/' &gt; gallery.html #生成html 相册find . -size 0 -exec rm '&#123;&#125;' \; #清理空文件find . -type d -exec mkdir -p $DESTDIR/&#123;&#125; \; #复制一个目录的结构，忽略文件find . -type f -size +500M -exec ls -ls &#123;&#125; \; | sort -n #找出所有大于500M的文件]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fasta 序列解析]]></title>
    <url>%2F2017%2F201705_parsefasta%2F</url>
    <content type="text"><![CDATA[fasta格式最常见，也最常用，也是分析中最简单的格式吧，以&gt;开始后续字符表示序列名称，空格后对序列进行描述（可能不存在），换行即为序列信息，可包括换行。下面是一个fasta格式序列实例： 1234&gt;gi|187608668|ref|NM_001043364.2| Bombyx mori moricin (Mor), mRNAAAACCGCGCAGTTATTTAAAATATGAATATTTTAAAACTTTTCTTTGTTTTTACAACGACAACTGTGTACTATTTTTTATATTTGGTTCGAAAAGTTGCATTATTAACGATTTTAGAAAATAAAACTACTTTACTTTTACACG 这个实例中序列名字为gi|187608668|ref|NM_001043364.2|，下面就直接贴上常用的fasta解析脚本： python手动解析123456789def parse_fasta(fa): seq = &#123;&#125; with oepn(fa) as fa: for i in fa: if i.startswith("&gt;"): name = i[1:].partition(' ')[0] elif: seq[name] = seq.get(name, '') + i.strip() return seq 使用HTSeq模块123def parse_fasta_by_HTSeq(fa): import HTSeq return &#123;i.name:i.seq for i in HTSeq.FastaReader(fa)&#125; 当然还能使用biopython进行解析，我不太常用这一模块，就不写了 fasta输出格式化12345def format_fasta(seq, len=60): tmp = '' for i in range((len(seq)//len)+1): tmp += '&#123;&#125;\n'.format(seq[(i*60):((i+1)*60)]) return tmp]]></content>
      <categories>
        <category>bioformat</category>
      </categories>
      <tags>
        <tag>fasta</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RefSeq 数据库 GFF 格式转换]]></title>
    <url>%2F2017%2F201705_RefSeqGFF2GTF%2F</url>
    <content type="text"><![CDATA[最近一项目被RefSeq数据库中gff注释格式折磨死，流程总是不能正确识别，各种方法改了格式，勉强运行，后续步骤继续报错，想着以后肯定还有更多这种情况，自己对文件格式的理解，写了格式转换脚本。 GFF和GTF格式非常类似，都被用来基因结构注释，由9列表示不同意义的特征组成，多行共同注释一个基因的详细结构。然而两者又有很大差异，脚本处理起来也是不同。相比而言GTF处理起来简单些，第三列feature可能是gene, transcript, exon, CDS这少数几种情况，最重要的描述基因类型名称的第九列attr很统一，都会包括一般常用的gene_id, gene_biotype, transcript_id, transcript_biotype等特征。而GFF文件包括一个明确的层级关系，gene是transcript（存在mRNA，lncRNA，ncRNA，rRNA，tRNA等多种不同组织方式）的父节点，而transcript又是exon和CDS的父节点，特征层级不同也决定了第九列attr的注释信息不同，gene id, name等信息也是层次关系关联，文件处理较麻烦。 NCBI RefSeq数据库中GFF注释格式感觉更是复杂怪异，废话不多说，直接上代码，愚蠢方法手撕GFF 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142#!/usr/bin/env python# -*- coding:utf-8 -*-import osimport sysimport gzipfrom collections import defaultdictfrom collections import Counterdef GFF_parser(file): openfile = gzip.open if file.endswith('gz') else open with openfile(file) as anno_file: for line in anno_file: if line.startswith('#'): continue seqname, source, feature, start, end, score, strand, \ frame, attributeStr = line.strip().split('\t') attribute = [i.strip() for i in attributeStr.split(';') if i.strip() != ''] attr = &#123;i.partition('=')[0]:i.partition('=')[2] for i in attribute&#125; yield seqname, source, feature, int(start), int(end), score, strand, frame, attrdef format_gtf_line(chro, feature, start, end, strand, attr, \ source=None, score=None, frame=None): source = source if source else 'RefSeq' score = score if score else '.' frame = frame if frame else '.' attrstr = ''.join(['&#123;&#125; "&#123;&#125;"; '.format(i[0], i[1]) for i in sorted(attr.items())]) gtf_line = '&#123;chro&#125;\t&#123;source&#125;\t&#123;feature&#125;\t&#123;start&#125;\t&#123;end&#125;\t&#123;score&#125;\t\ &#123;strand&#125;\t&#123;frame&#125;\t&#123;attrstr&#125;'.format(**locals()) return gtf_linedef main(gff, output): gtf_out = open(output, 'w')# RefSeq数据库中GFF文件feature收集，或去除无意义项，或替换 skip = ['region', 'miRNA', 'cDNA_match', 'repeat_region', 'D_loop', \ 'match', 'origin_of_replication', 'centromere', 'sequence_feature'] transcript = ['mRNA', 'lnc_RNA', 'primary_transcript', 'tRNA', 'rRNA', \ 'C_gene_segment', 'V_gene_segment', 'SRP_RNA', 'ncRNA', 'transcript'] biotype = ['lnc_RNA', 'tRNA', 'rRNA', 'SRP_RNA'] order = [] gene_info = defaultdict(dict) transcript_info = defaultdict(dict) check_gene_name = &#123;&#125; #检查重复gene_name check_transcript_name = &#123;&#125; for i in GFF_parser(gff): chro, source, feature, start, end, score, strand, frame, attr = i if feature in skip: continue elif feature == 'gene': gene_id = attr['ID'] gene_info[gene_id]['name'] = attr['Name'] gene_info[gene_id]['biotype'] = attr['gene_biotype'] check_gene_name[gene_id] = attr['Name'] #使用字典transcript_info储存转录本信息 elif feature in transcript: transcript_id = attr['ID'] gene_id = attr['Parent'] if 'Parent' in attr else transcript_id transcript_name = attr['transcript_id'] if 'transcript_id' \ in attr else transcript_id order.append(transcript_id) transcript_info[transcript_id]['gtf_info'] = [chro, start, end, strand] transcript_info[transcript_id]['name'] = transcript_name transcript_info[transcript_id]['gene'] = gene_id # 部分转录本不存在对应gene信息，直接关联转录类型 if feature in biotype: transcript_info[transcript_id]['biotype'] = feature check_transcript_name[transcript_id] = transcript_name elif feature == 'exon': transcript_id = attr['Parent'] try: transcript_info[transcript_id]['exon'][1].append(start) transcript_info[transcript_id]['exon'][2].append(end) except KeyError: transcript_info[transcript_id]['exon'] = [chro, [start,], \ [end,], strand] elif feature == 'CDS': transcript_id = attr['Parent'] try: transcript_info[transcript_id]['CDS'][1].append(start) transcript_info[transcript_id]['CDS'][2].append(end) transcript_info[transcript_id]['CDS'][4].append(frame) except KeyError: transcript_info[transcript_id]['CDS'] = [chro, [start,], \ [end,], strand, [frame,]] else: sys.stderr.write('Warning: Annotation Feature "&#123;&#125;" is not collected, '\ 'Unknown error may occur.\n'.format(feature))# 检查重复gene_name， transcript_names，进行替换 dup_name = lambda list_in: set([i[0] for i in Counter(list_in).items() if i[1] &gt; 1]) gene_name = [i[1] for i in check_gene_name.items()] dup_gene_name = dup_name(gene_name) transcript_name = [i[1] for i in check_transcript_name.items()] dup_transcript_name = dup_name(transcript_name)# 输出GTF文件，order控制输出顺序 for transcript in order: attr = &#123;&#125; gene_id = transcript_info[transcript]['gene'] try: gene_name = gene_info[gene_id]['name'] gene_name = gene_id if gene_name in dup_gene_name else gene_name except KeyError: gene_name = gene_id attr['gene_id'] = gene_name transcript_name = transcript_info[transcript]['name'] transcript_name = transcript if transcript_name in 、 dup_transcript_name else transcript_name attr['transcript_id'] = transcript_name try: attr['transcript_biotype'] = transcript_info[transcript]['biotype'] except KeyError: attr['transcript_biotype'] = gene_info[gene_id]['biotype'] try: attr['gene_biotype'] = gene_info[gene_id]['biotype'] except KeyError: attr['gene_biotype'] = transcript_info[transcript]['biotype'] chro, start, end, strand = transcript_info[transcript]['gtf_info'] gtf_out.write('&#123;&#125;\n'.format(format_gtf_line(chro,'transcript',、 start,end,strand,attr))) try: exon_feature = transcript_info[transcript]['exon'] except KeyError: exon_feature = transcript_info[transcript]['CDS'][:-1] chro, start, end, strand = exon_feature for i,j in enumerate(start): gtf_out.write('&#123;&#125;\n'.format(format_gtf_line(chro,'exon',、 start[i],end[i],strand,attr))) try: cds_feature = transcript_info[transcript]['CDS'] chro, start, end, strand, frame = cds_feature for i,j in enumerate(start): gtf_out.write('&#123;&#125;\n'.format(format_gtf_line(chro, 'CDS', start[i], \ end[i], strand, attr, frame=frame[i]))) except KeyError: continue #部分注释信息不完善，多次使用try... except...输出替代的对应注释信息 gtf_out.close() GFF格式包括gene, transcript, exon(CDS)三个层级，其中transcript进行上下关联最为重要，这里脚本以transcript作为键值进行数据处理，另外脚本还有一个假设就是gene, transcript对应ID在文件中唯一。]]></content>
      <categories>
        <category>bioformat</category>
      </categories>
      <tags>
        <tag>gtf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python 科学计算 —— anaconda]]></title>
    <url>%2F2017%2F201705_anaconda%2F</url>
    <content type="text"><![CDATA[python大法好，但是坑爹的版本，坑爹的包管理，折腾死大把大把工作时间。anaconda实现了python2和python3和谐共存，同时conda更方便的进行包管理，支持win，linux，mac多系统平台。 还有一些小小的便利，anaconda已经提供了python科学计算常用包，numpy, pandas, matplotlib等等，python notebook也必然是有的，目前已更名为jupyter notebook，输出文档很方便进行交流展示。 清华大学TUNA镜像网站提供了anaconda安装包，https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/，不必被下载速度折磨死。 anaconda安装和python一样，anaconda存在2和3两个版本，目前都更新到4.3.1版，但是，，，还有一个但是，选择安装哪一个版本并没有太大区别，可以方便的通过anaconda环境管理切换不同的python版本。 针对unix like用户，anaconda还有一个非常重要的特性，非root用户也能方便的进行安装(默认安装到~/anaconda2/)，添加~/anaconda2/bin到环境变量$PATH，而后即可享受anaconda。 12345678910#创建名为python3的环境，python版本为3.4，该命令仅安装python基本模块conda create --name python3 python=3.4#创建新环境，同时针对新环境安装anacondaconda create --name python3 python=3.4 anaconda#激活python3环境source activate python3#关闭环境source deactivate python3#删除环境conda remove --name python3 --all conda安装包conda于pip类似，对python包进行管理，还能管理python和conda本身 12345678#列出当前环境中已安装的包conda list #搜索，安装，升级，删除指定特定的包conda [search|install|update|remove] package_name#可以使用-n参数，对指定特定环境中的包进行操作conda list -n env_name#升级python或者conda，升级python至当前大版本的最新版本conda update python anaconda镜像设置anaconda主机在国外，conda在线安装包速度慢到怀疑人生，这时绝壁选择使用国内镜像网站，首先将仓库镜像加入conda配置，具体命令为： 1234# 添加Anaconda的TUNA镜像conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/# 设置搜索时显示通道地址conda config --set show_channel_urls yes 或者直接编辑文件~/.condarc 12345ssl_verify: truechannels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ - defaultsshow_channel_urls: true 然后，你就可以尽情享受python编程的快乐，不被各种包折磨死]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 ggplot 绘制扩增子 rank abundance 曲线]]></title>
    <url>%2F2017%2F201704_rankabundance%2F</url>
    <content type="text"><![CDATA[ggplot2实现微生物rank abundance绘图，另发现了reshape2能方便的进行dataframe格式整理，长表格，宽表格相互转换 具体代码实现如下： 123456789101112131415161718192021222324252627282930library(ggplot2)library(reshape2)# otu_table 抽平后的otu table，第一列为OTU编号，最后一列为tax注释otu_table &lt;- read.table(otu_table, sep="\t", header=TRUE)otu_table &lt;- otu_table[,2:(length(otu_table)-1)]for(i in 1:length(otu_table))&#123; # 计算等级相对丰度 sample &lt;- otu_table[,i] otu_table[,i]&lt;- rev(sort(sample/sum(sample)))&#125;rank_abundance &lt;- otu_tablerank_abundance$rank &lt;- 1:dim(rank_abundance)[1] # 添加等级# 宽表格整理成长表格dat &lt;- melt(rank_abundance, id.vars = c('rank'), variable.name='sample')# 整合样品分组信息group &lt;- read.table(group, sep='\t', col.names=c('sample', 'group'))dat &lt;- merge(dat, group, by=c('sample'), all=TRUE)# 移除0值dat &lt;- dat[dat$value&gt;0,]g &lt;- ggplot(dat, aes(rank, log10(dat$value), group=sample)) + geom_step(aes(color=group)) + #ylim(-4,0) + xlim(0,2000) + labs(x="Species Rank", y="Relative Abundance") + # 设置y轴指示刻度，针对不同数据，可能需要调整 scale_y_continuous(breaks=c(-4,-3,-2,-1), labels=c('1e-4','1e-3','1e-2','1e-1')) + # 调整y轴指示刻度显示方法 theme(axis.text.y = element_text(angle=90, hjust=0.5, vjust=1)) + ggsave("rank_abundance.pdf") 结果展示：]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>ggplot</tag>
        <tag>amplicon</tag>
        <tag>microbe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux 基本命令介绍]]></title>
    <url>%2F2017%2F201703_linuxcmd%2F</url>
    <content type="text"><![CDATA[linux 登录远程登录linux服务器具体原理呢，水平还不够，说不清，先简单粗暴上方法。一般采取ssh的方式远程登录，常用ssh客户端有xshell, putty，本地电脑与服务器间数据传输一般是ftp客户端的方式，可以使用xftp, FileZilla。 登录需要帐号，密码，主机地址(ip或可解析网址)，一般ssh登录端口选择22，ftp登录端口选择21。 basic command ls：列出当前路径（或特定路径）下的文件、文件夹 -l 先是完整信息，包括文件属主、时间、权限等信息 -a 列出包括隐藏文件在内的所有问价，linux下隐藏文件以.开头 -h 文件大小以人类友好的方式显示，直观结果为文件大小以k, m, g等代替bit -t 安装文件修改时间进行排列 pwd：显示当前所在路径 cd：进入文件夹，后接相对路径或全路径 mv：更名，移动文件 cp：拷贝文件 -L 拷贝源文件为链接时，追踪拷贝原始文件 -r 针对文件夹拷贝，递归拷贝文件夹下所有子目录或文件 ln：创建文件链接 -s 创建软链接，相比ln创建链接方式，ln -s创建链接不占用硬盘空间 mkdir：创建文件夹 -p 递归创建多级文件夹，eg：mkdir -p /path/to/a/b/c，其中文件夹a原本不存在，创建a同时创建子目录b子目录c rmdir：删除空文件夹 rm：删除文件或文件夹 -r 递归删除，通常用于删除非空文件夹 -f 强制删除，不显示提示消息 文件操作 cat, zcat：zcat支持打开压缩文件 head, tail：显示文件前10（默认）行，后10行 -n num 指定显示行数 more, less paste：横向合并两个文本 touch：创建文件 cut： -f n-m 显示文件第n到m列 -d 设置分隔符，默认‘\t’ join vim：文本编辑器 wc -l 统计文件行数 打包压缩 tar：文档打包，通过调用gzip或bzip2压缩 -c 打包，创建新的tar文件 -x 解包，从tar文件中提取文件 -f tar包文件 -v 显示当前正在处理的文件 -z 调用gzip进行压缩或解压缩 -j 调用bzip2进行压缩或解压缩 -J 调用xz进行压缩或解压缩 gzip：压缩和解压gz后缀文件 -d 解压gzip压缩文件 -c 压缩或解压结果输出到STDOUT -num [1-9] 数字越小压缩比率越低，速度越快，-1等效于-fast，-9等效于-best unzip：解压zip后缀文件 bzip2 权限管理 chmod：修改文件读写执行等权限，只能修改属主为自己的文件或文件夹 4, r代表read权限，2, w代表write权限，1, x代表execution权限 eg: chmod +x file 添加文件执行权限 ​ chomd 755 file 修改文件权限为属主读写执行，用户组读写，所有用户读写权限 chown：修改文件属主信息，eg：chown -R user:group file -R 递归修改子文件、目录属主信息 搜索 grep, zgrep：文件内容查找 -f file 查找pattern在file中 -v 取反，显示不包含pattern的行 -c 计数，输出包含pattern的行数 find：查找文件，指定文件路径下 locate：查找文件，所有硬盘内 awk：这个命令可以学，详细教程可参考https://github.com/mylxsw/growing-up/blob/master/doc/%E4%B8%89%E5%8D%81%E5%88%86%E9%92%9F%E5%AD%A6%E4%BC%9AAWK.md sed：这个也可以学，详细教程可参考https://github.com/mylxsw/growing-up/blob/master/doc/%E4%B8%89%E5%8D%81%E5%88%86%E9%92%9F%E5%AD%A6%E4%BC%9ASED.md 系统命令 top：显示进程 -c 显示详细命令 -u user 显示特定user进程 ps kill： kill进程 free：查看内存 history：查看shell执行历史记录 Linux系统命令可以通过man command或者command --help的方式查看详细帮助信息]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[parse pe fastq file use python]]></title>
    <url>%2F2017%2F201703_parsePEfastq%2F</url>
    <content type="text"><![CDATA[github看了业界大神li heng序列解析的脚本readfq，智商着急啊，没看懂，倒是新学了一个新功能，python生成器yield。 readfq.py适用于fasta或者单端fastq文件，但是目前测序主要以双端数据为主，现学现卖，实现了一个paried-end数据解析器，代码如下： 123456789101112131415161718192021222324252627#!/usr/bin/env python# --*-- coding:utf-8 --*--import osimport sysimport gzipdef parse_pe_fastq(fastq1,fastq2,phred=33): from numpy import fromstring,byte while True: name1 = fastq1.readline().partition(' ')[0] name2 = fastq2.readline().partition(' ')[0] if not name1 or not name2: break read1, nothing1, qual1 = fq1.readline()[:-1], fq1.readline(), fq1.readline()[:-1] read2, nothing2, qual2 = fq2.readline()[:-1], fq2.readline(), fq2.readline()[:-1] qual1 = fromstring(qual1,dtype=byte)-phred qual2 = fromstring(qual2,dtype=byte)-phred assert name1 == name2, 'fastq1, fastq2 is not paired-end' yield name1, read1, read2, qual1, qual2if __name__ == '__main__': fq1 = gzip.open(sys.argv[1],'r') fq1 = gzip.open(sys.argv[2],'r') for i in parse_pe_fastq(fq1, fq2): name, seq1, seq2, qua1, qua2 = i # filter, stat, split 通过while循环遍历双端文件，四行为一个单元进行数据读取，每四行作为一个完整单元返回（yeild），进行其他操作（# filter, stat, split）， 一个操作结束后迭代器（parse_pe_fastq）继续解析测序数据，for循环中qua1作为一个numpy.ndarray对象，可进行其他一些q20统计、过滤等操作。]]></content>
      <categories>
        <category>bioformat</category>
      </categories>
      <tags>
        <tag>fastq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA['XXX，从入门到放弃']]></title>
    <url>%2F2017%2F201703_GettingStarted%2F</url>
    <content type="text"><![CDATA[生物背景从事生物信息分析工作，本科时候对统计、计算机之类的有些兴趣，觉着“生物信息”很高大上的样子，什毛都不懂就如了生信的坑，这一路下来，知道越多越觉得需要了解、懂的东西也越多，经常有“我擦勒，要学这么多东西啊” ， linux, R, python, perl…… 微博上看了各种从入门到放弃系列，觉得很有意思，这里也不自知的写个从入门到放弃吧，生物信息是一个多学科的交叉学科，遗传、生理、计算机、统计基础好像都须知道一些，“XXX，从入门到放弃”，我也不知道是啥从入门到放弃。 很早就有利用博客整理、交流学习思路的想法，懒癌穷癌很成功的阻止了我，看到很多利用github的教程，简单粗暴，终于鼓起勇气试试。 讲真，统计不是很懂，遗传不知道怎么说起，那就说说生信分析的linux、R、python这些咯。 Linux分析主要工作环境，命令行操作，分析工具众多。 基础命令：cd，pwd，ls，mkdir，rm，touch，cp，mv，echo，wc，ln 文件查看：head，tail，more，less，cat (zcat) 打包压缩：tar，gzip，zip，bzip2 文件处理：vim，awk，sed，grep (zgrep)，join 其他几个有用的命令：find，top，ps，kill，free Linux系统命令可以通过man command或者command --help的方式查看帮助信息 其他几个须知的点：重定向&gt;，文本追加&gt;&gt;，管道|，后台运行&amp;和nohup，标准输入、标准输出和标准错误 R两个功能，分析和绘图。分析这个主要是利用一些现成R package，例如转录组分析中DESeq、edgeR这些；绘图，当时还是ggplot2，教程很多，我这个R战五渣没有发言权。学习R，当然要有RStudio，R书籍什么的， R语言实战、ggplot2数据分析与图形艺术都可以看下。 python主要是文件整理吧，不同格式转换，提取有效信息。python语法简单，基本上很快都能写出实现自己功能的代码，有很多第三方modules帮你实现想要的功能，另外学习的人多啊，不怕找不到人问问题。 编程能力这个还是靠多练习了，一些知识点就只能自己看书看教程背，现在网上也很多python相关的资料教程，边看边练习，入门很快的。记得，我当时看了很多七七八八的教程，然而少练习，折腾挺长时间，廖雪峰的python教程 (2.7)，笨方法学python ，github上也是有很多python学习笔记或者英文书籍翻译。 强烈推荐ipython，可以理解为python里的Rstudio吧，可以生成富文本(filename.ipynb)文件，很方便用于脚本展示、交流。 Anaconda ，一个python科学计算环境，包括了常用的numpy、pandas、matplotlib、ipython等modules，管理python工作环境，实现python2, python3和谐共存，这个也是刚开始了解，熟悉的朋友可以交流交流。 perl和python类似吧，文件整理。这个这个，只会打印hello, world，算是不会了，网上也很多博客教程，可以参考学习。 其他技能MarkDown语法简单，结果展示酷炫，低门槛装逼利器。 Typore：免费markdown编辑器里面最好的，界面简洁，支持多种格式（pdf，html，docx等等）导出。 Docker可以掌握的技能。关注docker，这是因为同事提到工作流程迁移系统环境兼容的问题，docker完全免去了这些烦恼。无奈，工作系统版本太低，无法安装，还没有试过。]]></content>
      <categories>
        <category>bioinfor</category>
      </categories>
  </entry>
</search>
